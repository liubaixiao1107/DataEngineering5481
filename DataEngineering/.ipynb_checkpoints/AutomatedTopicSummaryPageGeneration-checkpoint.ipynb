{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "168b54c8-d579-4a5d-9a69-be25bd7d6991",
   "metadata": {},
   "source": [
    "# Automated Topic Summary Page Generation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d91b2bba-182e-41fd-ad76-e0358d5899a9",
   "metadata": {},
   "source": [
    "## 1. Project Introduction\n",
    "This project automates the extraction, summarization, and presentation of keyword-related news events. It consists of several modules:\n",
    "\n",
    "1. **Data Cleaning (`cleaned_the_data`)** – cleans and standardizes raw news articles from `raw_news.json`, removing invalid entries, deduplicating using (title, link) hashes, stripping non-printable characters, normalizing text and dates, and producing `cleaned_news.json` as a reliable dataset.\n",
    "\n",
    "2. **Timeline Extraction (`extract_timeline`)** – groups news articles by date and generates one-line summaries of major events per day, using GPT-5 with prompts focused on the target keyword, producing a structured timeline.\n",
    "\n",
    "3. **Entity Extraction (`extract_entities`)** – extracts and normalizes people, organizations, and entities mentioned in the news dataset, generating structured JSON for downstream reporting.\n",
    "\n",
    "4. **Automated Summarization (`summary`)** – generates a three-paragraph narrative summary using an authoritative timeline and optional headlines, enforcing strict formatting rules and avoiding any fabrication of facts.\n",
    "\n",
    "5. **Report Generation (`generate_report`)** – compiles the summary, entities, timeline, and links into a visually appealing HTML report (`index.html`) with interactive cards and a responsive layout, accompanied by a self-contained CSS file for styling.\n",
    "\n",
    "The pipeline produces clean, structured, and human-readable outputs, enabling efficient analysis, dissemination, and visualization of key events and entities related to the target keyword."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd2cd3af-46a6-4f58-9457-3819c09931de",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-01T12:01:51.260380Z",
     "start_time": "2025-11-01T12:01:49.931073Z"
    }
   },
   "outputs": [],
   "source": [
    "# import libraries\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import time\n",
    "import random\n",
    "from datetime import datetime, timedelta, timezone\n",
    "from tqdm import tqdm\n",
    "import json\n",
    "import os\n",
    "import re\n",
    "from collections import Counter, defaultdict\n",
    "from typing import List, Dict, Set,Any, Tuple\n",
    "from difflib import SequenceMatcher\n",
    "from openai import OpenAI\n",
    "import argparse, os, json, re, time, logging, threading, socket, random\n",
    "from urllib.parse import urlparse\n",
    "from requests.adapters import HTTPAdapter\n",
    "from urllib3.util.retry import Retry\n",
    "from pathlib import Path\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6dbb7de-f840-4a2b-a339-568124b0652d",
   "metadata": {},
   "source": [
    "## 2. Crawl the news\n",
    "**crawl_the_news** is the first stage of the pipeline, responsible for automatically collecting news data and exporting it as structured JSON. It integrates **five major news APIs (NewsAPI, GNews, TheNewsAPI, CurrentsAPI, Mediastack)** to maximize coverage and reduce information gaps. The workflow includes: randomly splitting the time range to improve NewsAPI retrieval; calling each API’s fetch_xxx() function to collect titles, timestamps, and URLs; extracting the full article text using BeautifulSoup with extensive CSS selectors and a fallback paragraph-based method; and normalizing all articles into a unified {title, date, link, text} structure. The final output is saved as raw_news.json, which serves as the input for cleaning, deduplication, entity extraction, and summarization modules."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5914515-caab-4f99-b531-c2369e1bb3e3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-29T12:46:43.522001Z",
     "start_time": "2025-10-29T12:46:43.487454Z"
    }
   },
   "outputs": [],
   "source": [
    "# Key\n",
    "NewsAPI_Key = \"8406ef98a8b24bec854801aa9f2c6a35\"\n",
    "GNews_Key = \"9a6066514e3ca31d8ec6c184b2c33594\"\n",
    "TheNewsAPI_Key = \"wEj2kyyJhPKLICmZavDq2MeJgbOr1KcyLbU0X3Au\"\n",
    "CurrentsAPI_Key = \"wMSLtPfn74YOMCOyIGv49vXAfIrD2bcXGVgEj_zN1AgA8b3G\"\n",
    "Mediastack_Key = \"465890a7953f6a540676c7c0fb86508a\"\n",
    "\n",
    "# URL\n",
    "NewsAPI_URL = \"https://newsapi.org/v2/everything\"\n",
    "GNews_URL = \"https://gnews.io/api/v4/search\"\n",
    "TheNewsAPI_URL = \"https://api.thenewsapi.com/v1/news/all\"\n",
    "Mediastack_URL = \"http://api.mediastack.com/v1/news\"\n",
    "\n",
    "# json name\n",
    "raw_json = \"raw_news.json\"\n",
    "cleaned_json = \"cleaned_news.json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b20c7d30-5920-4be9-a8f1-1c4dbb1984e8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-29T12:46:43.542235Z",
     "start_time": "2025-10-29T12:46:43.532869Z"
    }
   },
   "outputs": [],
   "source": [
    "def extract_article_content(url):\n",
    "    \"\"\"\n",
    "    Extract main content from news webpage URL\n",
    "    \"\"\"\n",
    "    try:\n",
    "        headers = {\n",
    "            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36'\n",
    "        }\n",
    "        \n",
    "        response = requests.get(url, headers=headers, timeout=15)\n",
    "        response.raise_for_status()\n",
    "        \n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "        \n",
    "        # Remove unwanted tags\n",
    "        for tag in ['script', 'style', 'nav', 'header', 'footer', 'aside']:\n",
    "            for element in soup.find_all(tag):\n",
    "                element.decompose()\n",
    "        \n",
    "        # Content selectors for news websites\n",
    "        content_selectors = [\n",
    "            # Main content area\n",
    "            'article',\n",
    "            'main',\n",
    "            '.main-content',\n",
    "            '.content-main',\n",
    "            '#main-content',\n",
    "            '#content-main',\n",
    "            \n",
    "            # News specific selector\n",
    "            '.article',\n",
    "            '.story',\n",
    "            '.news-article',\n",
    "            '.post',\n",
    "            '.entry',\n",
    "            \n",
    "            # Main content\n",
    "            '.article-body',\n",
    "            '.story-body',\n",
    "            '.post-body',\n",
    "            '.entry-content',\n",
    "            '.article-content',\n",
    "            '.story-content',\n",
    "            '.post-content',\n",
    "            '.news-content',\n",
    "            '.content-body',\n",
    "            '.body-content',\n",
    "            \n",
    "            # text content\n",
    "            '.text-content',\n",
    "            '.article-text',\n",
    "            '.story-text',\n",
    "            '.post-text',\n",
    "            \n",
    "            # General Content\n",
    "            '[class*=\"content\"]',\n",
    "            '[class*=\"article\"]',\n",
    "            '[class*=\"story\"]',\n",
    "            '[class*=\"post\"]',\n",
    "            '[class*=\"entry\"]',\n",
    "            '[class*=\"body\"]',\n",
    "            '[class*=\"text\"]',\n",
    "            \n",
    "            # Specific news websites\n",
    "            '.zn-body__paragraph',  # CNN\n",
    "            '.caas-body',           # Yahoo News\n",
    "            '.Article__Content',    # Bloomberg\n",
    "            '.article-section',     # Reuters\n",
    "            '.article-page',        # BBC\n",
    "            '.story-wrapper',       # NBC\n",
    "            '.article-wrapper',\n",
    "            \n",
    "            # Container selector\n",
    "            '.container',\n",
    "            '.wrapper',\n",
    "            '.main',\n",
    "            '#main',\n",
    "            '#content',\n",
    "            '.page-content'\n",
    "        ]\n",
    "        \n",
    "        # Try selectors first\n",
    "        for selector in content_selectors:\n",
    "            elements = soup.select(selector)\n",
    "            for element in elements:\n",
    "                text = element.get_text(strip=True)\n",
    "                text = re.sub(r'\\s+', ' ', text)\n",
    "                if len(text) > 200:\n",
    "                    return text\n",
    "        \n",
    "        # Fallback: combine paragraphs\n",
    "        paragraphs = soup.find_all('p')\n",
    "        if paragraphs:\n",
    "            content = ' '.join(p.get_text(strip=True) for p in paragraphs if len(p.get_text(strip=True)) > 50)\n",
    "            content = re.sub(r'\\s+', ' ', content)\n",
    "            if len(content) > 100:\n",
    "                return content\n",
    "        \n",
    "        return \"No valid content extracted\"\n",
    "        \n",
    "    except Exception as e:\n",
    "        return f\"Error: {str(e)}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98e4d161-af70-4d2e-b7d7-38625b788305",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-29T12:46:43.560421Z",
     "start_time": "2025-10-29T12:46:43.555132Z"
    }
   },
   "outputs": [],
   "source": [
    "def save_to_json(data, filename='raw_news.json'):\n",
    "    try:\n",
    "        with open(filename, 'w', encoding='utf-8') as f:\n",
    "            json.dump(data, f, ensure_ascii=False, indent=2)\n",
    "        print(f\"The data has been saved to {filename}\")\n",
    "        return True\n",
    "    except Exception as e:\n",
    "        print(f\"Error saving file: {e}\")\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e3816cf-77c5-4947-b820-c0020acec6fe",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-29T12:46:43.582278Z",
     "start_time": "2025-10-29T12:46:43.570473Z"
    }
   },
   "outputs": [],
   "source": [
    "def fetch_news_from_newsapi(keyword, start_time, end_time):\n",
    "    params = {\n",
    "        'q': keyword,\n",
    "        'from': start_time,\n",
    "        'to': end_time,\n",
    "        'sortBy': 'publishedAt',\n",
    "        'pageSize': 100,\n",
    "        'language': 'en',\n",
    "        'apiKey': NewsAPI_Key\n",
    "    }\n",
    "    \n",
    "    try:\n",
    "        response = requests.get(NewsAPI_URL, params=params)\n",
    "        response.raise_for_status()\n",
    "        data = response.json()\n",
    "        articles = data.get('articles', [])\n",
    "        print(f\"Fetched {len(articles)} articles from NewsAPI\")\n",
    "        return articles\n",
    "    except Exception as e:\n",
    "        print(f\"NewsAPI request failed: {e}\")\n",
    "        return []\n",
    "\n",
    "def fetch_news_from_gnews(keyword, start_time, end_time):  \n",
    "    params = {\n",
    "        'q': keyword,\n",
    "        'from': start_time,\n",
    "        'to': end_time,\n",
    "        'max': 100,\n",
    "        'lang': 'en',\n",
    "        'token': GNews_Key\n",
    "    }\n",
    "    \n",
    "    try:\n",
    "        response = requests.get(GNews_URL, params=params)\n",
    "        response.raise_for_status()\n",
    "        data = response.json()\n",
    "        articles = data.get('articles', [])\n",
    "        print(f\"Fetched {len(articles)} articles from GNews\")\n",
    "        return articles\n",
    "    except Exception as e:\n",
    "        print(f\"GNews API request failed: {e}\")\n",
    "        return []\n",
    "\n",
    "def fetch_news_from_thenewsapi(keyword, start_time, end_time):\n",
    "    params = {\n",
    "        'api_token': TheNewsAPI_Key,\n",
    "        'search': keyword,\n",
    "        'published_after': start_time,\n",
    "        'language': 'en',\n",
    "        'limit': 100\n",
    "    }\n",
    "    \n",
    "    try:\n",
    "        response = requests.get(TheNewsAPI_URL, params=params)\n",
    "        response.raise_for_status()\n",
    "        data = response.json()\n",
    "        articles = data.get('data', [])\n",
    "        print(f\"Fetched {len(articles)} articles from The News API\")\n",
    "        return articles\n",
    "    except Exception as e:\n",
    "        print(f\"The News API request failed: {e}\")\n",
    "        return []\n",
    "\n",
    "def fetch_nobel_news_from_currentsapi(keyword, start_time, end_time):\n",
    "\n",
    "    start_time = datetime.strptime(start_time, \"%Y-%m-%d\").replace(tzinfo=timezone.utc)\n",
    "    end_time = datetime.strptime(end_time, \"%Y-%m-%d\").replace(tzinfo=timezone.utc)\n",
    "    \n",
    "    url = (f'https://api.currentsapi.services/v1/search?'\n",
    "           f'keywords={keyword}&language=en&'\n",
    "           f'apiKey={CurrentsAPI_Key}&'\n",
    "           f'start_date{start_time}&end_date{end_time}')\n",
    "    \n",
    "    try:\n",
    "        response = requests.get(url)\n",
    "        response.raise_for_status()\n",
    "        \n",
    "        data = response.json()\n",
    "        \n",
    "        if data.get('status') == 'ok':\n",
    "            articles = data.get('news', [])\n",
    "            print(f\"Fetched {len(articles)} articles from CurrentsAPI\")\n",
    "            return articles\n",
    "        else:\n",
    "            print(f\"CurrentsAPI returned error: {data.get('message', 'Unknown error')}\")\n",
    "            return []\n",
    "        \n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"CurrentsAPI request failed: {e}\")\n",
    "        return []\n",
    "    except json.JSONDecodeError:\n",
    "        print(\"Failed to parse CurrentsAPI response\")\n",
    "        return []\n",
    "    except Exception as e:\n",
    "        print(f\"Unexpected error: {e}\")\n",
    "        return []\n",
    "\n",
    "def fetch_news_from_mediastack(keyword, start_time, end_time):\n",
    "    params = {\n",
    "        'access_key': Mediastack_Key,\n",
    "        'keywords': keyword,\n",
    "        'languages': 'en',\n",
    "        'limit': 100,\n",
    "        'sort': 'published_desc',\n",
    "        'date': f'{start_time},{end_time}'\n",
    "    }\n",
    "    \n",
    "    try:\n",
    "        response = requests.get(Mediastack_URL, params=params)\n",
    "        data = response.json()\n",
    "        \n",
    "        if 'data' in data:\n",
    "            articles = data.get('data', [])\n",
    "            print(f\"Fetched {len(articles)} articles from Mediastack\")\n",
    "            return articles\n",
    "        else:\n",
    "            print(f\"Error: {data.get('error', 'Unknown error')}\")\n",
    "            return []\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"Mediastack API error: {e}\")\n",
    "        return []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3a2c36e-2703-4030-ae85-fafa8c49a0e9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-29T12:46:43.598341Z",
     "start_time": "2025-10-29T12:46:43.591052Z"
    }
   },
   "outputs": [],
   "source": [
    "def process_news_data(keyword, start_time, end_time):\n",
    "    print(\"Start obtaining news data...\")\n",
    "\n",
    "    raw_data = []\n",
    "    # \n",
    "    start_dt = datetime.strptime(start_time, \"%Y-%m-%d\")\n",
    "    end_dt = datetime.strptime(end_time, \"%Y-%m-%d\")\n",
    "    delta = end_dt - start_dt\n",
    "    random_days = random.randint(0, delta.days)\n",
    "    middle_time = (start_dt + timedelta(days=random_days)).strftime(\"%Y-%m-%d\")\n",
    "    \n",
    "    # Get data from all APIs\n",
    "    # Search twice\n",
    "    newsapi_articles_partone = fetch_news_from_newsapi(keyword, start_time, middle_time)\n",
    "    newsapi_articles_parttwo = fetch_news_from_newsapi(keyword, middle_time, end_time)\n",
    "    gnews_articles = fetch_news_from_gnews(keyword, start_time, end_time)\n",
    "    thenewsapi_articles = fetch_news_from_thenewsapi(keyword, start_time, end_time)\n",
    "    currents_articles = fetch_nobel_news_from_currentsapi(keyword, start_time, end_time)\n",
    "    # Search twice\n",
    "    mediastack_articles = fetch_news_from_mediastack(keyword, start_time, end_time)\n",
    "    \n",
    "    # Combine all articles\n",
    "    all_articles = []\n",
    "    all_articles.extend(newsapi_articles_partone)\n",
    "    all_articles.extend(newsapi_articles_parttwo)\n",
    "    all_articles.extend(gnews_articles)\n",
    "    all_articles.extend(thenewsapi_articles)\n",
    "    all_articles.extend(currents_articles)\n",
    "    all_articles.extend(mediastack_articles)\n",
    "    \n",
    "    print(f\"Total articles: {len(all_articles)}\")\n",
    "    \n",
    "    for i, article in enumerate(all_articles, 1):\n",
    "        print(f\"Processing {i}/{len(all_articles)}: {article['title'][:50]}...\")\n",
    "        \n",
    "        # Extract article content\n",
    "        text_content = extract_article_content(article['url'])\n",
    "        \n",
    "        # Build data structure\n",
    "        news_item = {\n",
    "            \"title\": article.get('title', 'No title'),\n",
    "            \"date\": article.get('publishedAt', 'No date'),\n",
    "            \"link\": article.get('url', ''),\n",
    "            \"text\": text_content\n",
    "        }\n",
    "        \n",
    "        raw_data.append(news_item)\n",
    "        \n",
    "        # Add delay to avoid rate limiting\n",
    "        time.sleep(1)\n",
    "    \n",
    "    return raw_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85dad55a-7778-4570-95c8-418d7b0acb48",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-29T12:46:43.612339Z",
     "start_time": "2025-10-29T12:46:43.607241Z"
    }
   },
   "outputs": [],
   "source": [
    "def crawl_the_news(keyword, start_time, end_time): \n",
    "    # Processing news data\n",
    "    raw_news_data = process_news_data(keyword, start_time, end_time)\n",
    "    \n",
    "    if raw_news_data:\n",
    "        # Save to JSON file\n",
    "        success = save_to_json(raw_news_data, raw_json)\n",
    "        \n",
    "        if success:\n",
    "            print(f\"Successfully processed {len(raw_news_data)} articles\")\n",
    "        else:\n",
    "            print(\"Failed to save file\")\n",
    "    else:\n",
    "        print(\"No data obtained\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff45c8cd-7fd1-446b-a5c2-dd74024238d1",
   "metadata": {},
   "source": [
    "## 3. Clean the data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55236166-3a35-457f-9d1d-191b873ca9c4",
   "metadata": {},
   "source": [
    "**cleaned_the_data** cleans and standardizes the raw dataset. It takes raw_news.json as input and produces a filtered, deduplicated, and normalized dataset cleaned_news.json. The module removes empty or invalid articles, eliminates duplicates using a (title, link) hash set, strips non-printable characters, converts article text to lowercase, and normalizes all dates to YYYY-MM-DD. Each cleaned record is stored in a unified structure {title, date, link, text}. The resulting cleaned_news.json serves as the clean and reliable input for downstream summarization, entity extraction, and timeline generation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e0f617a-f528-44c0-a6fd-683f167ac9dd",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-29T12:46:43.629138Z",
     "start_time": "2025-10-29T12:46:43.621230Z"
    }
   },
   "outputs": [],
   "source": [
    "# cleaned data structure:\n",
    "\n",
    "# cleaned_data_list = []\n",
    "# cleaned_data = {\n",
    "#     \"title\" : title\n",
    "#     \"date\" : date\n",
    "#     \"link\" : link\n",
    "#     \"text\" : text\n",
    "# }\n",
    "\n",
    "def cleaned_the_data():\n",
    "    # Load the original file\n",
    "    with open(raw_json, \"r\", encoding=\"utf-8\") as f:\n",
    "        data = json.load(f)\n",
    "    \n",
    "    cleaned = []\n",
    "    seen = set()\n",
    "    \n",
    "    for item in data:\n",
    "        title = item.get(\"title\", \"\").strip()\n",
    "        link = item.get(\"link\", \"\").strip()\n",
    "        text = item.get(\"text\", \"\").strip()\n",
    "        date_str = item.get(\"date\", \"\").strip()\n",
    "    \n",
    "        # Skip empty records or invalid text\n",
    "        if not title or not link or not text:\n",
    "            continue\n",
    "        if text.lower() == \"no valid content extracted\".lower():\n",
    "            continue\n",
    "    \n",
    "        # Skip duplicates\n",
    "        if (title, link) in seen:\n",
    "            continue\n",
    "        seen.add((title, link))\n",
    "    \n",
    "        # Remove gibberish or control characters (keep printable English/Chinese chars)\n",
    "        def clean_str(s):\n",
    "            return re.sub(r\"[^\\x09\\x0A\\x0D\\x20-\\x7E\\u4E00-\\u9FFF]\", \" \", s)\n",
    "    \n",
    "        title = clean_str(title)\n",
    "        text = clean_str(text).lower()  # convert all text to lowercase\n",
    "    \n",
    "        # Normalize date format to YYYY-MM-DD\n",
    "        if date_str:\n",
    "            try:\n",
    "                dt = datetime.fromisoformat(date_str.replace(\"Z\", \"+00:00\"))\n",
    "                date_str = dt.strftime(\"%Y-%m-%d\")\n",
    "            except Exception:\n",
    "                match = re.search(r\"(\\d{4})[-/](\\d{2})[-/](\\d{2})\", date_str)\n",
    "                if match:\n",
    "                    date_str = \"-\".join(match.groups())\n",
    "                else:\n",
    "                    date_str = \"\"\n",
    "    \n",
    "        cleaned.append({\n",
    "            \"title\": title,\n",
    "            \"date\": date_str,\n",
    "            \"link\": link,\n",
    "            \"text\": text.strip()\n",
    "        })\n",
    "    \n",
    "    # Save cleaned data\n",
    "    with open(\"../../assignment2/cleaned_news.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(cleaned, f, ensure_ascii=False, indent=2)\n",
    "    \n",
    "    print(f\"Cleaning completed. {len(cleaned)} valid news articles saved to cleaned_news.json.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bdedd3a-573d-4c75-bf1b-4835b615facf",
   "metadata": {},
   "source": [
    "## 4. Extract the information"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "117fd8db-1b25-46fc-8664-cd4d862d7b19",
   "metadata": {},
   "source": [
    "### extract_timeline \n",
    "generates a structured daily event timeline using GPT-5.\n",
    "It takes the cleaned dataset as input, groups all articles by date, and composes a compact text block for each day.\n",
    "A date-specific prompt is then sent to GPT-5, instructing the model to ignore all items unrelated to the specified keyword and produce exactly one sentence describing the key event of that day, including the main people, organizations, and time.\n",
    "Each returned summary is stored in the unified structure {date, event}.\n",
    "The resulting timeline list provides a concise, date-ordered narrative foundation for the final summary webpage.\n",
    "\n",
    "### extract_entities\n",
    "extract_entities extracts and normalizes all keyword-related entities from the cleaned dataset using GPT-5.\n",
    "For each article, a prompt is sent instructing the model to identify and canonicalize names of people, organizations, and prizes, and return them in strict JSON format.\n",
    "The module parses each JSON response and merges all extracted entries into a unified list following the structure {people, organizations, prize}.\n",
    "The resulting entity set forms the structured knowledge base used for the final summary page’s key-entity section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52a9ac94-e1aa-46c8-8688-3db2a62309fc",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-01T11:56:15.667947Z",
     "start_time": "2025-11-01T11:56:15.667640Z"
    }
   },
   "outputs": [],
   "source": [
    "# timeline structure\n",
    "\n",
    "# timeline_list = []\n",
    "# timeline = {\n",
    "#     \"date\" : date\n",
    "#     \"event\" : event\n",
    "# }\n",
    "\n",
    "# entities.json\n",
    "\n",
    "# entities_list = []\n",
    "# entity = {\n",
    "#     \"people\" : people\n",
    "#     \"prize\" : prize\n",
    "#     \"organizations\" organizaitions\n",
    "# }\n",
    "\n",
    "OPENAI_API_KEY=\"test\"\n",
    "\n",
    "client = OpenAI(api_key=OPENAI_API_KEY)\n",
    "\n",
    "# ---------------------------------------------------------------------\n",
    "# 1. Extract timeline\n",
    "# ---------------------------------------------------------------------\n",
    "def extract_timeline(news,keyword):\n",
    "    \"\"\"\n",
    "    Group articles by date and ask GPT-5 to summarize each day's major event.\n",
    "    Returns:\n",
    "        timeline_list = [\n",
    "            {\"date\": \"YYYY-MM-DD\", \"event\": \"...\"},\n",
    "            ...\n",
    "        ]\n",
    "    \"\"\"\n",
    "    # group by date\n",
    "    by_date = defaultdict(list)\n",
    "    for item in news:\n",
    "        d = item.get(\"date\")\n",
    "        if d:\n",
    "            by_date[d].append(item)\n",
    "\n",
    "    timeline_list = []\n",
    "    # process each date, oldest to newest,come up with one-line summary\n",
    "    for d, items in sorted(by_date.items(), key=lambda x: x[0]):\n",
    "        # prepare short text for the model\n",
    "        joined = \"\\n\\n\".join([\n",
    "            f\"- Title: {it.get('title','')}\\n  Text: {it.get('text','')}\"\n",
    "            for it in items\n",
    "        ])\n",
    "\n",
    "        system = (\n",
    "            f\"\"\"\n",
    "            You are an expert summarizer. Based only on the provided file content, without searching the web,\n",
    "            remove all news items that are not directly related to the {keyword}\n",
    "            For each remaining Nobel-related event, compress it into exactly one sentence.\n",
    "            Each sentence must clearly state the key person(s), organization(s), and time.\n",
    "            Return the final set of one-sentence events only.\n",
    "            \"\"\"\n",
    "            # \"\"\"\n",
    "            # You are given several text messages that were recorded on the same day.\n",
    "            # Your task is to identify and extract the distinct events mentioned in these messages.\n",
    "            # \"\"\"\n",
    "        )\n",
    "        user = f\"Date: {d}\\nNews snippets:\\n{joined}\\n\\nReturn only the final one-line event.\"\n",
    "\n",
    "        response = client.responses.create(\n",
    "            model=\"gpt-5\",\n",
    "            input=[\n",
    "                {\"role\": \"system\", \"content\": system},\n",
    "                {\"role\": \"user\", \"content\": user},\n",
    "            ]\n",
    "        )\n",
    "        content=response.output_text\n",
    "        timeline_list.append({\"date\": d, \"event\": content})\n",
    "\n",
    "    return timeline_list\n",
    "\n",
    "\n",
    "# ---------------------------------------------------------------------\n",
    "# 2. Extract entities\n",
    "# ---------------------------------------------------------------------\n",
    "\n",
    "def extract_entities(news,keyword):\n",
    "    \"\"\"\n",
    "    Ask GPT-5 to extract and normalize people, organizations, and prizes\n",
    "    from the entire dataset.\n",
    "    Returns:\n",
    "        entities_dict = {\n",
    "            \"people\": [...],\n",
    "            \"organizations\": [...],\n",
    "            \"prize\": [...]\n",
    "        }\n",
    "    \"\"\"\n",
    "\n",
    "    entities_list = []\n",
    "    for item in news:\n",
    "        # concat limited sample of texts for prompt (avoid overly long input)\n",
    "        joined = \"\\n\\n\".join([\n",
    "            f\"- Title: {item.get('title','')}\\n  Text: {item.get('text','')}\"\n",
    "        ])\n",
    "        user = (\n",
    "            f\"\"\"\n",
    "            Extract and normalize{keyword}-related named entities from the following articles.\n",
    "            Return a JSON array where each element has fields: 'people', 'organizations', 'prize'.\n",
    "            Each entry should contain unique canonical names.\n",
    "            Ensure the output is strictly valid JSON\\n\\n\n",
    "            \"Articles:\\n{joined}\n",
    "            \"\"\"\n",
    "        )\n",
    "        response = client.responses.create(\n",
    "            model=\"gpt-5\",\n",
    "            input=[{\"role\": \"user\", \"content\": user}],\n",
    "            # text_format=Entities\n",
    "        )\n",
    "        result = json.loads(response.output_text)\n",
    "        entities_list.extend(result)\n",
    "\n",
    "    return entities_list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3480ffb6-a71b-4ea8-9931-ca670ea78289",
   "metadata": {},
   "source": [
    "## 5. Summarize"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "963edd94-0da2-4488-a9f0-04ab54135c86",
   "metadata": {},
   "source": [
    "**summary()** generates a three-paragraph English narrative summary of the 2025 Nobel Prizes. It takes cleaned_news.json as input and optionally uses timeline.json as the authoritative timeline, producing final_summary.txt in the specified output directory. The function filters news for keyword-related items, constructs system and user prompts, and calls the API https://open.bigmodel.cn/api/paas/v4/chat/completions via call_glm(). The system prompt (OVERALL_PROMPT) enforces strict format, style, and no-fabrication rules, while the user prompt provides the timeline and news headlines, instructing the model to output exactly three paragraphs. The response is processed to ensure three paragraphs; extra paragraphs are truncated and fewer paragraphs are refactored using refactor_to_three_paragraphs(). AdaptiveLimiter manages request rate and handles 429, timeout, or other errors. The final output aligns with the timeline and ignores non-Nobel or non-2025 events, providing high-quality text for downstream analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "848e9461-535c-4ff6-a7c6-68bdb501440d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-29T12:46:43.652712Z",
     "start_time": "2025-10-29T12:46:43.649645Z"
    }
   },
   "outputs": [],
   "source": [
    "API_URL = \"https://open.bigmodel.cn/api/paas/v4/chat/completions\"\n",
    "\n",
    "logging.basicConfig(level=logging.INFO,\n",
    "    format=\"%(asctime)s | %(levelname)s | %(message)s\", datefmt=\"%H:%M:%S\")\n",
    "log = logging.getLogger(\"topic2\")\n",
    "\n",
    "def ensure_dir(p): os.makedirs(p, exist_ok=True); return p\n",
    "def load_json(p):\n",
    "    with open(p, \"r\", encoding=\"utf-8\") as f: return json.load(f)\n",
    "def domain_of(url: str) -> str:\n",
    "    try: return urlparse(url).netloc or \"\"\n",
    "    except: return \"\"\n",
    "def is_refusal(text: str) -> bool:\n",
    "    if not text: return True\n",
    "\n",
    "\n",
    "_SENT_SPLIT = re.compile(r'(?<=[。！？!?\\.])\\s+(?=[A-Z“\"(\\[]|[A-Z][a-z])')\n",
    "def split_sentences(text: str):\n",
    "    t = re.sub(r'\\s+', ' ', (text or \"\").strip())\n",
    "    parts = _SENT_SPLIT.split(t)\n",
    "    if len(parts) <= 1:\n",
    "        parts = re.split(r'(?<=[\\.!?])\\s+', t)\n",
    "    return [s.strip() for s in parts if s.strip()]\n",
    "\n",
    "def refactor_to_three_paragraphs(text: str):\n",
    "    sents = split_sentences(text)\n",
    "    if not sents: return (text or \"\").strip()\n",
    "    n = len(sents)\n",
    "    if n <= 3:\n",
    "        p1 = \" \".join(sents[:1]); p2 = \" \".join(sents[1:2]); p3 = \" \".join(sents[2:])\n",
    "        return \"\\n\\n\".join([p for p in (p1,p2,p3) if p]).strip()\n",
    "    p1_len = min(5, max(3, n//6 or 3))\n",
    "    rem = n - p1_len\n",
    "    p2_len = min(10, max(6, rem//2 or 6))\n",
    "    p3_len = n - p1_len - p2_len\n",
    "    if p3_len < 3 and n >= 12:\n",
    "        move = min(3 - p3_len, p2_len - 6)\n",
    "        if move > 0:\n",
    "            p2_len -= move\n",
    "            p3_len += move\n",
    "    p1 = \" \".join(sents[:p1_len]).strip()\n",
    "    p2 = \" \".join(sents[p1_len:p1_len+p2_len]).strip()\n",
    "    p3 = \" \".join(sents[p1_len+p2_len:]).strip()\n",
    "    return \"\\n\\n\".join([p for p in (p1,p2,p3) if p]).strip()\n",
    "\n",
    "_NOBEL_PAT = re.compile(r\"\\bNobel\\b|\", re.IGNORECASE)\n",
    "_YEAR_2025_PAT = re.compile(r\"\\b2025\\b|2025\")\n",
    "\n",
    "def is_nobel_related(title: str, text: str = \"\") -> bool:\n",
    "    t = (title or \"\").strip()\n",
    "    if not t: return False\n",
    "    return bool(_NOBEL_PAT.search(t) or _NOBEL_PAT.search(text or \"\"))\n",
    "\n",
    "def is_year_2025(title: str, date_str: str, text: str = \"\") -> bool:\n",
    "    if _YEAR_2025_PAT.search(title or \"\") or _YEAR_2025_PAT.search(text or \"\"):\n",
    "        return True\n",
    "    if (date_str or \"\").startswith(\"2025-\"):\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "class AdaptiveLimiter:\n",
    "    def __init__(self, qps: float = 0.5, min_qps: float = 0.15, max_qps: float = 1.2):\n",
    "        self.lock = threading.Lock(); self.qps=qps; self.min_qps=min_qps; self.max_qps=max_qps\n",
    "        self.last = 0.0; self.cool_until = 0.0\n",
    "    def wait(self):\n",
    "        with self.lock:\n",
    "            now = time.monotonic()\n",
    "            if now < self.cool_until:\n",
    "                time.sleep(self.cool_until - now); now = time.monotonic()\n",
    "            interval = 1.0 / max(self.qps, self.min_qps)\n",
    "            delta = interval - (now - self.last)\n",
    "            if delta > 0: time.sleep(delta); now = time.monotonic()\n",
    "            self.last = now\n",
    "    def punish_429(self):\n",
    "        with self.lock:\n",
    "            self.qps = max(self.min_qps, self.qps * 0.6)\n",
    "            cool = 6.0 + random.random()*6.0\n",
    "            self.cool_until = time.monotonic() + cool\n",
    "            log.warning(f\"set off 429：slowdown {self.qps:.2f} QPS，and freeze {cool:.1f}s\")\n",
    "    def punish_timeout(self):\n",
    "        with self.lock:\n",
    "            self.qps = max(self.min_qps, self.qps * 0.8)\n",
    "            log.warning(f\"quest over time ：slowdown {self.qps:.2f} QPS\")\n",
    "\n",
    "def make_session():\n",
    "    s = requests.Session()\n",
    "    retry = Retry(\n",
    "        total=3, connect=3, read=3,\n",
    "        backoff_factor=1.2,\n",
    "        status_forcelist=[429, 500, 502, 503, 504],\n",
    "        allowed_methods=[\"POST\"]\n",
    "    )\n",
    "    adapter = HTTPAdapter(max_retries=retry, pool_maxsize=2)\n",
    "    s.mount(\"https://\", adapter); s.mount(\"http://\", adapter)\n",
    "    s.headers.update({\"Accept\":\"application/json\", \"Connection\":\"close\"})\n",
    "    return s\n",
    "\n",
    "def call_glm(session: requests.Session, limiter: AdaptiveLimiter, api_key: str, messages,\n",
    "             model=\"glm-4.5-flash\", temperature=0.26, max_tokens=1400,\n",
    "             timeout=120, max_retries=3):\n",
    "    headers = {\"Authorization\": f\"Bearer {api_key}\", \"Content-Type\": \"application/json\"}\n",
    "    payload = {\"model\": model, \"messages\": messages, \"temperature\": temperature,\n",
    "               \"max_tokens\": max_tokens, \"stream\": False}\n",
    "    last_err = None\n",
    "    for attempt in range(max_retries + 1):\n",
    "        limiter.wait()\n",
    "        try:\n",
    "            resp = session.post(API_URL, headers=headers, json=payload, timeout=timeout)\n",
    "            if resp.status_code == 200:\n",
    "                j = resp.json()\n",
    "                txt = j.get(\"choices\", [{}])[0].get(\"message\", {}).get(\"content\", \"\")\n",
    "                return txt\n",
    "            if resp.status_code == 429:\n",
    "                limiter.punish_429()\n",
    "                time.sleep(1.2 + random.random()); continue\n",
    "            if resp.status_code in (408, 500, 502, 503, 504):\n",
    "                wait = (1.8 + random.random()) * (2 ** attempt)\n",
    "                log.warning(f\"default {resp.status_code}：{resp.text[:160]}...，{wait:.1f}s retry\")\n",
    "                time.sleep(wait); continue\n",
    "            raise RuntimeError(f\"HTTP {resp.status_code}: {resp.text[:500]}\")\n",
    "        except (requests.Timeout, socket.timeout) as e:\n",
    "            last_err = e; limiter.punish_timeout()\n",
    "            if attempt < max_retries:\n",
    "                wait = (1.2 + random.random()) * (2 ** attempt)\n",
    "                log.warning(f\"quest over time，{wait:.1f}s retry\")\n",
    "                time.sleep(wait); continue\n",
    "            break\n",
    "        except requests.RequestException as e:\n",
    "            last_err = e; break\n",
    "    raise RuntimeError(f\"model unusable：{last_err or 'unknown error'}\")\n",
    "\n",
    "# ---------------- main ----------------\n",
    "def summary(keyword):\n",
    "    #prompts\n",
    "    TIMELINE_PREAMBLE = (\n",
    "        \"Here is an authoritative timeline (date + event) that MUST be treated as ground truth. \"\n",
    "        \"When headlines conflict, resolve in favor of the timeline. \"\n",
    "        \"Do NOT invent prizewinners or entities not supported by the timeline/headlines.\"\n",
    "    )\n",
    "    \n",
    "    OVERALL_PROMPT = f\"\"\"\n",
    "        You will receive:\n",
    "        1) An authoritative timeline of the {keyword} (ground truth).\n",
    "        2) Optionally, a list of headlines (title, publisher domain, date).\n",
    "        \n",
    "        Your task: write a **single English narrative summary** ONLY about the **{keyword}**.\n",
    "        \n",
    "        STRICT FORMAT & STYLE:\n",
    "        • **Output EXACTLY THREE PARAGRAPHS**, with a blank line between paragraphs.\n",
    "        • Paragraph 1 (3–5 sentences): concise highlights in flowing prose — e.g.,\n",
    "          “the {keyword} honored key contributors, highlighting their impact in relevant fields.\n",
    "           Use such phrasing **only if these facts are supported**; otherwise use generic wording without adding specifics.”\n",
    "        • Paragraph 2 (~200–300 words): an integrative overview linking breakthroughs and societal meaning.\n",
    "        • Paragraph 3 (~180–260 words): synthesize 3–5 cross-cutting themes with transitions (meanwhile, in turn, as a result, by contrast…).\n",
    "          Do **not** enumerate by date or outlet.\n",
    "        \n",
    "        HARD CONSTRAINTS:\n",
    "        • **NO FABRICATION**: do not invent winners, categories, dates, affiliations, numbers, or methods.\n",
    "        • **TIMELINE-ALIGNED**: if any conflict arises, prefer the timeline; otherwise generalize.\n",
    "        • **FOCUS**: Ignore any non-{keyword} or non-2025 items entirely.\n",
    "        • Output plain text only (no JSON, no headers, no lists).\n",
    "        \"\"\"\n",
    "\n",
    "    ap = argparse.ArgumentParser()\n",
    "    ap.add_argument(\"--input\", default=\"cleaned_news.json\")\n",
    "    ap.add_argument(\"--timeline\", default=\"timeline.json\")\n",
    "    ap.add_argument(\"--output_dir\", default=\"outputs\")\n",
    "    ap.add_argument(\"--model\", default=\"glm-4.5-flash\")\n",
    "    ap.add_argument(\"--api-key\", default=\"d0b8bc52cf6b4c368982dfdd32384757.UcWBjZr72H7AWgyN\")\n",
    "    ap.add_argument(\"--qps\", type=float, default=0.5)\n",
    "    ap.add_argument(\"--use-headlines\", action=\"store_true\")\n",
    "    args = ap.parse_args()\n",
    "\n",
    "    ensure_dir(args.output_dir)\n",
    "\n",
    "    # load timeline\n",
    "    if not args.timeline or not os.path.exists(args.timeline):\n",
    "        log.warning(\"can not find timeline ，try headlines\")\n",
    "        timeline = []\n",
    "    else:\n",
    "        timeline = load_json(args.timeline)\n",
    "        if not isinstance(timeline, list):\n",
    "            log.warning(\" \"); timeline = []\n",
    "        else:\n",
    "            log.info(f\"loaded timeline：{args.timeline}（{len(timeline)} ）\")\n",
    "    # load news\n",
    "    if not os.path.exists(args.input):\n",
    "        log.error(f\"none input file：{args.input}\"); return\n",
    "    data = load_json(args.input)\n",
    "    log.info(f\"load data：{args.input}，total：{len(data)}\")\n",
    "\n",
    "    nobel_items = []\n",
    "    for it in data:\n",
    "        title = (it.get(\"title\") or \"\").strip()\n",
    "        text  = (it.get(\"text\")  or \"\")\n",
    "        date  = (it.get(\"date\")  or \"unknown\").strip() or \"unknown\"\n",
    "        if not title: continue\n",
    "        if is_nobel_related(title, text) and is_year_2025(title, date, text):\n",
    "            nobel_items.append({\"date\": date, \"title\": title, \"source\": domain_of(it.get(\"link\",\"\") or \"\")})\n",
    "\n",
    "    if not timeline and not nobel_items:\n",
    "        out = os.path.join(args.output_dir, \"final_summary.txt\")\n",
    "        with open(out, \"w\", encoding=\"utf-8\") as f:\n",
    "            f.write(f\"No{keyword}–related timeline or headlines were found.\")\n",
    "        log.info(f\"written：{out}\"); return\n",
    "\n",
    "    system_prompt = OVERALL_PROMPT\n",
    "    blocks = []\n",
    "\n",
    "    if timeline:\n",
    "        blocks.append(\"TIMELINE (authoritative):\\n\" + json.dumps(timeline, ensure_ascii=False, indent=2))\n",
    "\n",
    "    if args.use_headlines and nobel_items:\n",
    "        items_blob = json.dumps({\"items\": sorted(nobel_items, key=lambda x: x['date'])}, ensure_ascii=False)\n",
    "        blocks.append(\"HEADLINES (secondary evidence):\\n\" + items_blob)\n",
    "\n",
    "    user_msg = (\n",
    "        \"Use the timeline as ground truth. If any conflict arises, prefer the timeline.\\n\\n\"\n",
    "        + (\"\\n\\n\".join(blocks) if blocks else \"No timeline provided; rely on headlines without fabrication.\")\n",
    "        + \"\\n\\nRemember: Output EXACTLY THREE PARAGRAPHS separated by a blank line. \"\n",
    "          \"Do not list dates or outlets; write flowing prose.\"\n",
    "    )\n",
    "\n",
    "    session = make_session()\n",
    "    limiter = AdaptiveLimiter(qps=args.qps)\n",
    "\n",
    "    try:\n",
    "        raw = call_glm(session, limiter, args.api_key or os.getenv(\"ZHIPU_API_KEY\",\"\"),\n",
    "                       [{\"role\":\"system\",\"content\":system_prompt},\n",
    "                        {\"role\":\"user\",\"content\":user_msg}],\n",
    "                       model=args.model, temperature=0.26, max_tokens=2400, timeout=120)\n",
    "    except Exception as e:\n",
    "        log.warning(f\"generate fail：{e}\")\n",
    "        raw = \"\"\n",
    "\n",
    "    if is_refusal(raw):\n",
    "        final = (\"could not generate\")\n",
    "    else:\n",
    "        paras = [p for p in re.split(r'\\n\\s*\\n', raw.strip()) if p.strip()]\n",
    "        final = (\"\\n\\n\".join(re.sub(r'\\s+',' ', p).strip() for p in paras[:3])\n",
    "                 if len(paras) >= 3 else refactor_to_three_paragraphs(raw))\n",
    "\n",
    "    out_path = os.path.join(args.output_dir, \"final_summary.txt\")\n",
    "    with open(out_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(final)\n",
    "    log.info(f\"written：{out_path}\\n finish\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5abd0f8-9e3b-4b01-9076-a23963b47e5d",
   "metadata": {},
   "source": [
    "## 6. Generate HTML Page\n",
    "**generate_report** generates a comprehensive HTML report from preprocessed inputs. It takes four input **files—summary_file, entities_file, timeline_file, and cleaned_news**—and produces a self-contained report index.html with accompanying style.css. The function reads the summary text, structured entities, chronological events, and reference links, then organizes them into visually distinct sections: a hero header with the report title, a summary section, entity cards listing people and organizations, timeline cards sorted by date, and a grid of clickable link buttons. CSS styling ensures a clean, responsive, and interactive presentation, including hover effects, shadows, and consistent spacing. The resulting index.html and style.css provide a readable and visually appealing report suitable for display, sharing, or further analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ed36189-8088-42f8-8ac2-e814ac874c05",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-29T12:46:43.662110Z",
     "start_time": "2025-10-29T12:46:43.659072Z"
    }
   },
   "outputs": [],
   "source": [
    "def generate_report(summary_file, entities_file, timeline_file, link_file, report_title=\"Comprehensive Report\"):\n",
    "    # -------------------\n",
    "    # Read file\n",
    "    # -------------------\n",
    "    with open(summary_file, \"r\", encoding=\"utf-8\") as f:\n",
    "        summary_text = f.read().strip()\n",
    "\n",
    "    with open(entities_file, \"r\", encoding=\"utf-8\") as f:\n",
    "        entities = json.load(f)\n",
    "\n",
    "    with open(timeline_file, \"r\", encoding=\"utf-8\") as f:\n",
    "        timeline = json.load(f)\n",
    "\n",
    "    with open(link_file, \"r\", encoding=\"utf-8\") as f:\n",
    "        links = json.load(f)\n",
    "\n",
    "    # -------------------\n",
    "    # HTML\n",
    "    # -------------------\n",
    "    html_content = f\"\"\"<!DOCTYPE html>\n",
    "<html lang=\"en\">\n",
    "<head>\n",
    "<meta charset=\"UTF-8\">\n",
    "<meta name=\"viewport\" content=\"width=device-width, initial-scale=1.0\">\n",
    "<title>{report_title}</title>\n",
    "<link rel=\"stylesheet\" href=\"bootstrap.min.css\">\n",
    "<link rel=\"stylesheet\" href=\"style.css\">\n",
    "</head>\n",
    "<body>\n",
    "<section class=\"hero-section text-white text-center py-5\">\n",
    "    <div class=\"container\">\n",
    "        <h1>{report_title}</h1>\n",
    "    </div>\n",
    "</section>\n",
    "\n",
    "<section id=\"summary\" class=\"py-5\">\n",
    "<div class=\"container\">\n",
    "    <div class=\"section-header text-center mb-5\">\n",
    "        <h2>Summary</h2>\n",
    "    </div>\n",
    "    <p>{summary_text}</p>\n",
    "</div>\n",
    "</section>\n",
    "\n",
    "<section id=\"entities\" class=\"py-5 bg-light\">\n",
    "<div class=\"container\">\n",
    "    <div class=\"section-header text-center mb-5\">\n",
    "        <h2>Entities</h2>\n",
    "    </div>\n",
    "    <div class=\"row g-4\">\n",
    "\"\"\"\n",
    "\n",
    "    # Entities card\n",
    "    for ent in entities:\n",
    "        name = ent.get(\"name\", \"\")\n",
    "        category = ent.get(\"category\", \"\")\n",
    "        people = \"\".join(f\"<li>{p}</li>\" for p in ent.get(\"people\", []))\n",
    "        orgs = \"\".join(f\"<li>{o}</li>\" for o in ent.get(\"organizations\", []))\n",
    "\n",
    "        html_content += f\"\"\"\n",
    "        <div class=\"col-md-6 col-lg-4\">\n",
    "            <div class=\"entity-card p-4 mb-3\">\n",
    "                <h5>{name} {f'({category})' if category else ''}</h5>\n",
    "                <ul class=\"entity-list\">{people}</ul>\n",
    "                {('<h6>Organizations</h6><ul class=\"entity-list\">'+orgs+'</ul>') if orgs else ''}\n",
    "            </div>\n",
    "        </div>\n",
    "    \"\"\"\n",
    "\n",
    "    html_content += \"\"\"\n",
    "    </div>\n",
    "</div>\n",
    "</section>\n",
    "\n",
    "<section id=\"timeline\" class=\"py-5\">\n",
    "<div class=\"container\">\n",
    "    <div class=\"section-header text-center mb-5\">\n",
    "        <h2>Timeline</h2>\n",
    "    </div>\n",
    "\"\"\"\n",
    "\n",
    "    # Timeline card\n",
    "    timeline_sorted = sorted(timeline, key=lambda x: x[\"date\"])\n",
    "    for t in timeline_sorted:\n",
    "        date_str = datetime.strptime(t[\"date\"], \"%Y-%m-%d\").strftime(\"%B %d, %Y\")\n",
    "        html_content += f\"\"\"\n",
    "    <div class=\"timeline-item p-3 mb-4\">\n",
    "        <span class=\"timeline-date\">{date_str}</span>\n",
    "        <p>{t['event']}</p>\n",
    "    </div>\n",
    "    \"\"\"\n",
    "\n",
    "    html_content += \"\"\"\n",
    "</div>\n",
    "</section>\n",
    "\n",
    "<section id=\"links\" class=\"py-5 bg-light\">\n",
    "<div class=\"container\">\n",
    "    <div class=\"section-header text-center mb-4\">\n",
    "        <h2>Links</h2>\n",
    "    </div>\n",
    "    <div class=\"link-grid\">\n",
    "\"\"\"\n",
    "\n",
    "    # Link button\n",
    "    for l in links:\n",
    "        html_content += f\"\"\"\n",
    "        <a href=\"{l['link']}\" target=\"_blank\" class=\"link-btn\">\n",
    "            <h5>{l['title']}</h5>\n",
    "        </a>\n",
    "        \"\"\"\n",
    "\n",
    "    html_content += \"\"\"\n",
    "    </div>\n",
    "</div>\n",
    "</section>\n",
    "\n",
    "<script src=\"bootstrap.bundle.min.js\"></script>\n",
    "</body>\n",
    "</html>\n",
    "\"\"\"\n",
    "\n",
    "    Path(\"index.html\").write_text(html_content, encoding=\"utf-8\")\n",
    "\n",
    "    # -------------------\n",
    "    # CSS\n",
    "    # -------------------\n",
    "    css_content = \"\"\"\n",
    "body {\n",
    "    font-family:'Segoe UI',sans-serif;\n",
    "    background:#f4f5f7;\n",
    "    margin:0;\n",
    "    padding-bottom:100px; \n",
    "}\n",
    ".hero-section {\n",
    "    background: linear-gradient(135deg,#0d6efd,#6610f2);\n",
    "    color:white;\n",
    "    position:relative;\n",
    "    overflow:hidden;\n",
    "}\n",
    ".hero-section::after {\n",
    "    content:'';\n",
    "    position:absolute;\n",
    "    top:0; left:0; right:0; bottom:0;\n",
    "    background:rgba(0,0,0,0.2);\n",
    "}\n",
    ".section-header h2 {\n",
    "    font-size:2.5rem; margin-bottom:1rem;\n",
    "    position:relative; display:inline-block;\n",
    "}\n",
    ".section-header h2::after {\n",
    "    content:''; position:absolute;\n",
    "    bottom:-10px; left:50%; transform:translateX(-50%);\n",
    "    width:60px; height:4px; background:#0d6efd; border-radius:2px;\n",
    "}\n",
    ".entity-card {\n",
    "    background:white;\n",
    "    border-left:4px solid #6610f2;\n",
    "    border-radius:12px;\n",
    "    transition:all 0.4s;\n",
    "    box-shadow:0 2px 20px rgba(0,0,0,0.05);\n",
    "}\n",
    ".entity-card:hover {\n",
    "    transform:translateY(-8px);\n",
    "    box-shadow:0 8px 30px rgba(0,0,0,0.15);\n",
    "}\n",
    ".entity-list { list-style:none; padding:0; }\n",
    ".entity-list li::before {\n",
    "    content:'▸'; margin-right:10px; color:#6610f2; font-weight:bold;\n",
    "}\n",
    ".timeline-item { background:white; padding:20px; margin-bottom:15px; border-radius:12px; box-shadow:0 4px 15px rgba(0,0,0,0.1); }\n",
    ".timeline-date { display:block; font-weight:600; color:#6610f2; margin-bottom:8px; }\n",
    ".link-grid {\n",
    "    display:grid;\n",
    "    grid-template-columns: repeat(auto-fit, minmax(280px, 1fr));\n",
    "    gap:20px;\n",
    "    justify-items:center;\n",
    "}\n",
    ".link-btn {\n",
    "    display:block;\n",
    "    background:white;\n",
    "    padding:20px;\n",
    "    border-radius:12px;\n",
    "    box-shadow:0 4px 15px rgba(0,0,0,0.1);\n",
    "    text-align:center;\n",
    "    text-decoration:none;\n",
    "    color:#000;\n",
    "    transition:all 0.3s;\n",
    "}\n",
    ".link-btn:hover {\n",
    "    transform:translateY(-5px);\n",
    "    box-shadow:0 8px 25px rgba(0,0,0,0.15);\n",
    "}\n",
    "\"\"\"\n",
    "\n",
    "    Path(\"style.css\").write_text(css_content, encoding=\"utf-8\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34d750ca-5c8f-42c7-87b0-7e929fe3e319",
   "metadata": {},
   "source": [
    "## 7. Run pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7be47e0a-5dea-45d2-a922-11a2afc17bb4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-01T11:55:16.882439Z",
     "start_time": "2025-11-01T11:55:16.879297Z"
    }
   },
   "outputs": [],
   "source": [
    "# Input event name:\n",
    "keyword = \"Nobel Prize\" # 2025 Nobel Prize\n",
    "# Input start time:\n",
    "start_time = \"2025-09-23\" # “2025-09-23” \n",
    "# Input end time:\n",
    "end_time = \"2025-10-22\" # “2025-10-22”"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a344d13-4d9c-4135-b266-164ca2a6164f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crawl the news\n",
    "crawl_the_news(keyword, start_time, end_time)\n",
    "# Clean the data\n",
    "cleaned_the_data()\n",
    "\n",
    "# Extract information\n",
    "with open(\"cleaned_news.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    data = json.load(f)\n",
    "# Extract timeline\n",
    "timeline = extract_timeline(data,keyword)\n",
    "# save results\n",
    "json.dump(timeline, open(\"timeline.json\", \"w\", encoding=\"utf-8\"), ensure_ascii=False, indent=2)\n",
    "# Extract entities\n",
    "entities = extract_entities(data,keyword)\n",
    "# save results\n",
    "json.dump(entities, open(\"entities.json\", \"w\", encoding=\"utf-8\"), ensure_ascii=False, indent=2)\n",
    "\n",
    "# Summarize\n",
    "summary(keyword)\n",
    "\n",
    "# Generate HTML page\n",
    "generate_report(\"final_summary.txt\", \"entities.json\", \"timeline.json\", \"cleaned_news.json\", report_title=\"Comprehensive Report\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
