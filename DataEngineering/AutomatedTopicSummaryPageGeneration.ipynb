{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "168b54c8-d579-4a5d-9a69-be25bd7d6991",
   "metadata": {},
   "source": [
    "# Automated Topic Summary Page Generation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d91b2bba-182e-41fd-ad76-e0358d5899a9",
   "metadata": {},
   "source": [
    "## 1. Project Introduction"
   ]
  },
  {
   "cell_type": "code",
   "id": "fd2cd3af-46a6-4f58-9457-3819c09931de",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-01T12:01:51.260380Z",
     "start_time": "2025-11-01T12:01:49.931073Z"
    }
   },
   "source": [
    "# import libraries\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import time\n",
    "import random\n",
    "from datetime import datetime, timedelta, timezone\n",
    "from tqdm import tqdm\n",
    "import json\n",
    "import os\n",
    "import re\n",
    "from collections import Counter, defaultdict\n",
    "from typing import List, Dict, Set,Any, Tuple\n",
    "from difflib import SequenceMatcher\n",
    "from openai import OpenAI\n"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "cell_type": "markdown",
   "id": "b6dbb7de-f840-4a2b-a339-568124b0652d",
   "metadata": {},
   "source": [
    "## 2. Crawl the news"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd89323f-f311-40ef-9a4d-371d870af621",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "id": "a5914515-caab-4f99-b531-c2369e1bb3e3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-29T12:46:43.522001Z",
     "start_time": "2025-10-29T12:46:43.487454Z"
    }
   },
   "source": [
    "# Key\n",
    "NewsAPI_Key = \"8406ef98a8b24bec854801aa9f2c6a35\"\n",
    "GNews_Key = \"9a6066514e3ca31d8ec6c184b2c33594\"\n",
    "TheNewsAPI_Key = \"wEj2kyyJhPKLICmZavDq2MeJgbOr1KcyLbU0X3Au\"\n",
    "CurrentsAPI_Key = \"wMSLtPfn74YOMCOyIGv49vXAfIrD2bcXGVgEj_zN1AgA8b3G\"\n",
    "Mediastack_Key = \"465890a7953f6a540676c7c0fb86508a\"\n",
    "\n",
    "# URL\n",
    "NewsAPI_URL = \"https://newsapi.org/v2/everything\"\n",
    "GNews_URL = \"https://gnews.io/api/v4/search\"\n",
    "TheNewsAPI_URL = \"https://api.thenewsapi.com/v1/news/all\"\n",
    "Mediastack_URL = \"http://api.mediastack.com/v1/news\"\n",
    "\n",
    "# json name\n",
    "raw_json = \"raw_news.json\"\n",
    "cleaned_json = \"cleaned_news.json\""
   ],
   "outputs": [],
   "execution_count": 14
  },
  {
   "cell_type": "code",
   "id": "b20c7d30-5920-4be9-a8f1-1c4dbb1984e8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-29T12:46:43.542235Z",
     "start_time": "2025-10-29T12:46:43.532869Z"
    }
   },
   "source": [
    "def extract_article_content(url):\n",
    "    \"\"\"\n",
    "    Extract main content from news webpage URL\n",
    "    \"\"\"\n",
    "    try:\n",
    "        headers = {\n",
    "            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36'\n",
    "        }\n",
    "        \n",
    "        response = requests.get(url, headers=headers, timeout=15)\n",
    "        response.raise_for_status()\n",
    "        \n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "        \n",
    "        # Remove unwanted tags\n",
    "        for tag in ['script', 'style', 'nav', 'header', 'footer', 'aside']:\n",
    "            for element in soup.find_all(tag):\n",
    "                element.decompose()\n",
    "        \n",
    "        # Content selectors for news websites\n",
    "        content_selectors = [\n",
    "            # Main content area\n",
    "            'article',\n",
    "            'main',\n",
    "            '.main-content',\n",
    "            '.content-main',\n",
    "            '#main-content',\n",
    "            '#content-main',\n",
    "            \n",
    "            # News specific selector\n",
    "            '.article',\n",
    "            '.story',\n",
    "            '.news-article',\n",
    "            '.post',\n",
    "            '.entry',\n",
    "            \n",
    "            # Main content\n",
    "            '.article-body',\n",
    "            '.story-body',\n",
    "            '.post-body',\n",
    "            '.entry-content',\n",
    "            '.article-content',\n",
    "            '.story-content',\n",
    "            '.post-content',\n",
    "            '.news-content',\n",
    "            '.content-body',\n",
    "            '.body-content',\n",
    "            \n",
    "            # text content\n",
    "            '.text-content',\n",
    "            '.article-text',\n",
    "            '.story-text',\n",
    "            '.post-text',\n",
    "            \n",
    "            # General Content\n",
    "            '[class*=\"content\"]',\n",
    "            '[class*=\"article\"]',\n",
    "            '[class*=\"story\"]',\n",
    "            '[class*=\"post\"]',\n",
    "            '[class*=\"entry\"]',\n",
    "            '[class*=\"body\"]',\n",
    "            '[class*=\"text\"]',\n",
    "            \n",
    "            # Specific news websites\n",
    "            '.zn-body__paragraph',  # CNN\n",
    "            '.caas-body',           # Yahoo News\n",
    "            '.Article__Content',    # Bloomberg\n",
    "            '.article-section',     # Reuters\n",
    "            '.article-page',        # BBC\n",
    "            '.story-wrapper',       # NBC\n",
    "            '.article-wrapper',\n",
    "            \n",
    "            # Container selector\n",
    "            '.container',\n",
    "            '.wrapper',\n",
    "            '.main',\n",
    "            '#main',\n",
    "            '#content',\n",
    "            '.page-content'\n",
    "        ]\n",
    "        \n",
    "        # Try selectors first\n",
    "        for selector in content_selectors:\n",
    "            elements = soup.select(selector)\n",
    "            for element in elements:\n",
    "                text = element.get_text(strip=True)\n",
    "                text = re.sub(r'\\s+', ' ', text)\n",
    "                if len(text) > 200:\n",
    "                    return text\n",
    "        \n",
    "        # Fallback: combine paragraphs\n",
    "        paragraphs = soup.find_all('p')\n",
    "        if paragraphs:\n",
    "            content = ' '.join(p.get_text(strip=True) for p in paragraphs if len(p.get_text(strip=True)) > 50)\n",
    "            content = re.sub(r'\\s+', ' ', content)\n",
    "            if len(content) > 100:\n",
    "                return content\n",
    "        \n",
    "        return \"No valid content extracted\"\n",
    "        \n",
    "    except Exception as e:\n",
    "        return f\"Error: {str(e)}\""
   ],
   "outputs": [],
   "execution_count": 15
  },
  {
   "cell_type": "code",
   "id": "98e4d161-af70-4d2e-b7d7-38625b788305",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-29T12:46:43.560421Z",
     "start_time": "2025-10-29T12:46:43.555132Z"
    }
   },
   "source": [
    "def save_to_json(data, filename='raw_news.json'):\n",
    "    try:\n",
    "        with open(filename, 'w', encoding='utf-8') as f:\n",
    "            json.dump(data, f, ensure_ascii=False, indent=2)\n",
    "        print(f\"The data has been saved to {filename}\")\n",
    "        return True\n",
    "    except Exception as e:\n",
    "        print(f\"Error saving file: {e}\")\n",
    "        return False"
   ],
   "outputs": [],
   "execution_count": 16
  },
  {
   "cell_type": "code",
   "id": "9e3816cf-77c5-4947-b820-c0020acec6fe",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-29T12:46:43.582278Z",
     "start_time": "2025-10-29T12:46:43.570473Z"
    }
   },
   "source": [
    "def fetch_news_from_newsapi(keyword, start_time, end_time):\n",
    "    params = {\n",
    "        'q': keyword,\n",
    "        'from': start_time,\n",
    "        'to': end_time,\n",
    "        'sortBy': 'publishedAt',\n",
    "        'pageSize': 100,\n",
    "        'language': 'en',\n",
    "        'apiKey': NewsAPI_Key\n",
    "    }\n",
    "    \n",
    "    try:\n",
    "        response = requests.get(NewsAPI_URL, params=params)\n",
    "        response.raise_for_status()\n",
    "        data = response.json()\n",
    "        articles = data.get('articles', [])\n",
    "        print(f\"Fetched {len(articles)} articles from NewsAPI\")\n",
    "        return articles\n",
    "    except Exception as e:\n",
    "        print(f\"NewsAPI request failed: {e}\")\n",
    "        return []\n",
    "\n",
    "def fetch_news_from_gnews(keyword, start_time, end_time):  \n",
    "    params = {\n",
    "        'q': keyword,\n",
    "        'from': start_time,\n",
    "        'to': end_time,\n",
    "        'max': 100,\n",
    "        'lang': 'en',\n",
    "        'token': GNews_Key\n",
    "    }\n",
    "    \n",
    "    try:\n",
    "        response = requests.get(GNews_URL, params=params)\n",
    "        response.raise_for_status()\n",
    "        data = response.json()\n",
    "        articles = data.get('articles', [])\n",
    "        print(f\"Fetched {len(articles)} articles from GNews\")\n",
    "        return articles\n",
    "    except Exception as e:\n",
    "        print(f\"GNews API request failed: {e}\")\n",
    "        return []\n",
    "\n",
    "def fetch_news_from_thenewsapi(keyword, start_time, end_time):\n",
    "    params = {\n",
    "        'api_token': TheNewsAPI_Key,\n",
    "        'search': keyword,\n",
    "        'published_after': start_time,\n",
    "        'language': 'en',\n",
    "        'limit': 100\n",
    "    }\n",
    "    \n",
    "    try:\n",
    "        response = requests.get(TheNewsAPI_URL, params=params)\n",
    "        response.raise_for_status()\n",
    "        data = response.json()\n",
    "        articles = data.get('data', [])\n",
    "        print(f\"Fetched {len(articles)} articles from The News API\")\n",
    "        return articles\n",
    "    except Exception as e:\n",
    "        print(f\"The News API request failed: {e}\")\n",
    "        return []\n",
    "\n",
    "def fetch_nobel_news_from_currentsapi(keyword, start_time, end_time):\n",
    "\n",
    "    start_time = datetime.strptime(start_time, \"%Y-%m-%d\").replace(tzinfo=timezone.utc)\n",
    "    end_time = datetime.strptime(end_time, \"%Y-%m-%d\").replace(tzinfo=timezone.utc)\n",
    "    \n",
    "    url = (f'https://api.currentsapi.services/v1/search?'\n",
    "           f'keywords={keyword}&language=en&'\n",
    "           f'apiKey={CurrentsAPI_Key}&'\n",
    "           f'start_date{start_time}&end_date{end_time}')\n",
    "    \n",
    "    try:\n",
    "        response = requests.get(url)\n",
    "        response.raise_for_status()\n",
    "        \n",
    "        data = response.json()\n",
    "        \n",
    "        if data.get('status') == 'ok':\n",
    "            articles = data.get('news', [])\n",
    "            print(f\"Fetched {len(articles)} articles from CurrentsAPI\")\n",
    "            return articles\n",
    "        else:\n",
    "            print(f\"CurrentsAPI returned error: {data.get('message', 'Unknown error')}\")\n",
    "            return []\n",
    "        \n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"CurrentsAPI request failed: {e}\")\n",
    "        return []\n",
    "    except json.JSONDecodeError:\n",
    "        print(\"Failed to parse CurrentsAPI response\")\n",
    "        return []\n",
    "    except Exception as e:\n",
    "        print(f\"Unexpected error: {e}\")\n",
    "        return []\n",
    "\n",
    "def fetch_news_from_mediastack(keyword, start_time, end_time):\n",
    "    params = {\n",
    "        'access_key': Mediastack_Key,\n",
    "        'keywords': keyword,\n",
    "        'languages': 'en',\n",
    "        'limit': 100,\n",
    "        'sort': 'published_desc',\n",
    "        'date': f'{start_time},{end_time}'\n",
    "    }\n",
    "    \n",
    "    try:\n",
    "        response = requests.get(Mediastack_URL, params=params)\n",
    "        data = response.json()\n",
    "        \n",
    "        if 'data' in data:\n",
    "            articles = data.get('data', [])\n",
    "            print(f\"Fetched {len(articles)} articles from Mediastack\")\n",
    "            return articles\n",
    "        else:\n",
    "            print(f\"Error: {data.get('error', 'Unknown error')}\")\n",
    "            return []\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"Mediastack API error: {e}\")\n",
    "        return []"
   ],
   "outputs": [],
   "execution_count": 17
  },
  {
   "cell_type": "code",
   "id": "a3a2c36e-2703-4030-ae85-fafa8c49a0e9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-29T12:46:43.598341Z",
     "start_time": "2025-10-29T12:46:43.591052Z"
    }
   },
   "source": [
    "def process_news_data(keyword, start_time, end_time):\n",
    "    print(\"Start obtaining news data...\")\n",
    "\n",
    "    raw_data = []\n",
    "    # \n",
    "    start_dt = datetime.strptime(start_time, \"%Y-%m-%d\")\n",
    "    end_dt = datetime.strptime(end_time, \"%Y-%m-%d\")\n",
    "    delta = end_dt - start_dt\n",
    "    random_days = random.randint(0, delta.days)\n",
    "    middle_time = (start_dt + timedelta(days=random_days)).strftime(\"%Y-%m-%d\")\n",
    "    \n",
    "    # Get data from all APIs\n",
    "    # Search twice\n",
    "    newsapi_articles_partone = fetch_news_from_newsapi(keyword, start_time, middle_time)\n",
    "    newsapi_articles_parttwo = fetch_news_from_newsapi(keyword, middle_time, end_time)\n",
    "    gnews_articles = fetch_news_from_gnews(keyword, start_time, end_time)\n",
    "    thenewsapi_articles = fetch_news_from_thenewsapi(keyword, start_time, end_time)\n",
    "    currents_articles = fetch_nobel_news_from_currentsapi(keyword, start_time, end_time)\n",
    "    # Search twice\n",
    "    mediastack_articles = fetch_news_from_mediastack(keyword, start_time, end_time)\n",
    "    \n",
    "    # Combine all articles\n",
    "    all_articles = []\n",
    "    all_articles.extend(newsapi_articles_partone)\n",
    "    all_articles.extend(newsapi_articles_parttwo)\n",
    "    all_articles.extend(gnews_articles)\n",
    "    all_articles.extend(thenewsapi_articles)\n",
    "    all_articles.extend(currents_articles)\n",
    "    all_articles.extend(mediastack_articles)\n",
    "    \n",
    "    print(f\"Total articles: {len(all_articles)}\")\n",
    "    \n",
    "    for i, article in enumerate(all_articles, 1):\n",
    "        print(f\"Processing {i}/{len(all_articles)}: {article['title'][:50]}...\")\n",
    "        \n",
    "        # Extract article content\n",
    "        text_content = extract_article_content(article['url'])\n",
    "        \n",
    "        # Build data structure\n",
    "        news_item = {\n",
    "            \"title\": article.get('title', 'No title'),\n",
    "            \"date\": article.get('publishedAt', 'No date'),\n",
    "            \"link\": article.get('url', ''),\n",
    "            \"text\": text_content\n",
    "        }\n",
    "        \n",
    "        raw_data.append(news_item)\n",
    "        \n",
    "        # Add delay to avoid rate limiting\n",
    "        time.sleep(1)\n",
    "    \n",
    "    return raw_data"
   ],
   "outputs": [],
   "execution_count": 18
  },
  {
   "cell_type": "code",
   "id": "85dad55a-7778-4570-95c8-418d7b0acb48",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-29T12:46:43.612339Z",
     "start_time": "2025-10-29T12:46:43.607241Z"
    }
   },
   "source": [
    "def crawl_the_news(keyword, start_time, end_time): \n",
    "    # Processing news data\n",
    "    raw_news_data = process_news_data(keyword, start_time, end_time)\n",
    "    \n",
    "    if raw_news_data:\n",
    "        # Save to JSON file\n",
    "        success = save_to_json(raw_news_data, raw_json)\n",
    "        \n",
    "        if success:\n",
    "            print(f\"Successfully processed {len(raw_news_data)} articles\")\n",
    "        else:\n",
    "            print(\"Failed to save file\")\n",
    "    else:\n",
    "        print(\"No data obtained\")"
   ],
   "outputs": [],
   "execution_count": 19
  },
  {
   "cell_type": "markdown",
   "id": "ff45c8cd-7fd1-446b-a5c2-dd74024238d1",
   "metadata": {},
   "source": [
    "## 3. Clean the data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55236166-3a35-457f-9d1d-191b873ca9c4",
   "metadata": {},
   "source": [
    "\n",
    "input: raw_news.json\n",
    "<br/>output: cleaned_news.json"
   ]
  },
  {
   "cell_type": "code",
   "id": "4e0f617a-f528-44c0-a6fd-683f167ac9dd",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-29T12:46:43.629138Z",
     "start_time": "2025-10-29T12:46:43.621230Z"
    }
   },
   "source": [
    "# cleaned data structure:\n",
    "\n",
    "# cleaned_data_list = []\n",
    "# cleaned_data = {\n",
    "#     \"title\" : title\n",
    "#     \"date\" : date\n",
    "#     \"link\" : link\n",
    "#     \"text\" : text\n",
    "# }\n",
    "\n",
    "def cleaned_the_data():\n",
    "    # Load the original file\n",
    "    with open(raw_json, \"r\", encoding=\"utf-8\") as f:\n",
    "        data = json.load(f)\n",
    "    \n",
    "    cleaned = []\n",
    "    seen = set()\n",
    "    \n",
    "    for item in data:\n",
    "        title = item.get(\"title\", \"\").strip()\n",
    "        link = item.get(\"link\", \"\").strip()\n",
    "        text = item.get(\"text\", \"\").strip()\n",
    "        date_str = item.get(\"date\", \"\").strip()\n",
    "    \n",
    "        # Skip empty records or invalid text\n",
    "        if not title or not link or not text:\n",
    "            continue\n",
    "        if text.lower() == \"no valid content extracted\".lower():\n",
    "            continue\n",
    "    \n",
    "        # Skip duplicates\n",
    "        if (title, link) in seen:\n",
    "            continue\n",
    "        seen.add((title, link))\n",
    "    \n",
    "        # Remove gibberish or control characters (keep printable English/Chinese chars)\n",
    "        def clean_str(s):\n",
    "            return re.sub(r\"[^\\x09\\x0A\\x0D\\x20-\\x7E\\u4E00-\\u9FFF]\", \" \", s)\n",
    "    \n",
    "        title = clean_str(title)\n",
    "        text = clean_str(text).lower()  # convert all text to lowercase\n",
    "    \n",
    "        # Normalize date format to YYYY-MM-DD\n",
    "        if date_str:\n",
    "            try:\n",
    "                dt = datetime.fromisoformat(date_str.replace(\"Z\", \"+00:00\"))\n",
    "                date_str = dt.strftime(\"%Y-%m-%d\")\n",
    "            except Exception:\n",
    "                match = re.search(r\"(\\d{4})[-/](\\d{2})[-/](\\d{2})\", date_str)\n",
    "                if match:\n",
    "                    date_str = \"-\".join(match.groups())\n",
    "                else:\n",
    "                    date_str = \"\"\n",
    "    \n",
    "        cleaned.append({\n",
    "            \"title\": title,\n",
    "            \"date\": date_str,\n",
    "            \"link\": link,\n",
    "            \"text\": text.strip()\n",
    "        })\n",
    "    \n",
    "    # Save cleaned data\n",
    "    with open(\"../../assignment2/cleaned_news.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(cleaned, f, ensure_ascii=False, indent=2)\n",
    "    \n",
    "    print(f\"✅ Cleaning completed. {len(cleaned)} valid news articles saved to cleaned_news.json.\")"
   ],
   "outputs": [],
   "execution_count": 20
  },
  {
   "cell_type": "markdown",
   "id": "6bdedd3a-573d-4c75-bf1b-4835b615facf",
   "metadata": {},
   "source": [
    "## 4. Extract the information"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "117fd8db-1b25-46fc-8664-cd4d862d7b19",
   "metadata": {},
   "source": [
    "input:cleaned_news.json\n",
    "<br/>output:timeline.json, entities.json\n",
    "<br/>NLP+LLM"
   ]
  },
  {
   "cell_type": "code",
   "id": "52a9ac94-e1aa-46c8-8688-3db2a62309fc",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-01T11:56:15.667947Z",
     "start_time": "2025-11-01T11:56:15.667640Z"
    }
   },
   "source": [
    "# timeline structure\n",
    "\n",
    "# timeline_list = []\n",
    "# timeline = {\n",
    "#     \"date\" : date\n",
    "#     \"event\" : event\n",
    "# }\n",
    "\n",
    "# entities.json\n",
    "\n",
    "# entities_list = []\n",
    "# entity = {\n",
    "#     \"people\" : people\n",
    "#     \"prize\" : prize\n",
    "#     \"organizations\" organizaitions\n",
    "# }\n",
    "\n",
    "OPENAI_API_KEY=\"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\"\n",
    "\n",
    "client = OpenAI(api_key=OPENAI_API_KEY)\n",
    "\n",
    "# ---------------------------------------------------------------------\n",
    "# 1. Extract timeline\n",
    "# ---------------------------------------------------------------------\n",
    "def extract_timeline(news):\n",
    "    \"\"\"\n",
    "    Group articles by date and ask GPT-5 to summarize each day's major event.\n",
    "    Returns:\n",
    "        timeline_list = [\n",
    "            {\"date\": \"YYYY-MM-DD\", \"event\": \"...\"},\n",
    "            ...\n",
    "        ]\n",
    "    \"\"\"\n",
    "    # group by date\n",
    "    by_date = defaultdict(list)\n",
    "    for item in news:\n",
    "        d = item.get(\"date\")\n",
    "        if d:\n",
    "            by_date[d].append(item)\n",
    "\n",
    "    timeline_list = []\n",
    "    # process each date, oldest to newest,come up with one-line summary\n",
    "    for d, items in sorted(by_date.items(), key=lambda x: x[0]):\n",
    "        # prepare short text for the model\n",
    "        joined = \"\\n\\n\".join([\n",
    "            f\"- Title: {it.get('title','')}\\n  Text: {it.get('text','')}\"\n",
    "            for it in items\n",
    "        ])\n",
    "\n",
    "        system = (\n",
    "            \"\"\"\n",
    "            You are an expert data summarizer. Given several news snippets\n",
    "            about events on the SAME date, produce ONE concise\n",
    "            timeline entry.\n",
    "            \"\"\"\n",
    "            # \"\"\"\n",
    "            # You are given several text messages that were recorded on the same day.\n",
    "            # Your task is to identify and extract the distinct events mentioned in these messages.\n",
    "            # \"\"\"\n",
    "        )\n",
    "        user = f\"Date: {d}\\nNews snippets:\\n{joined}\\n\\nReturn only the final one-line event.\"\n",
    "\n",
    "        response = client.responses.create(\n",
    "            model=\"gpt-5\",\n",
    "            input=[\n",
    "                {\"role\": \"system\", \"content\": system},\n",
    "                {\"role\": \"user\", \"content\": user},\n",
    "            ]\n",
    "        )\n",
    "        content=response.output_text\n",
    "        timeline_list.append({\"date\": d, \"event\": content})\n",
    "\n",
    "    return timeline_list\n",
    "\n",
    "\n",
    "# ---------------------------------------------------------------------\n",
    "# 2. Extract entities\n",
    "# ---------------------------------------------------------------------\n",
    "\n",
    "def extract_entities(news):\n",
    "    \"\"\"\n",
    "    Ask GPT-5 to extract and normalize people, organizations, and prizes\n",
    "    from the entire dataset.\n",
    "    Returns:\n",
    "        entities_dict = {\n",
    "            \"people\": [...],\n",
    "            \"organizations\": [...],\n",
    "            \"prize\": [...]\n",
    "        }\n",
    "    \"\"\"\n",
    "\n",
    "    entities_list = []\n",
    "    for item in news:\n",
    "        # concat limited sample of texts for prompt (avoid overly long input)\n",
    "        joined = \"\\n\\n\".join([\n",
    "            f\"- Title: {item.get('title','')}\\n  Text: {item.get('text','')}\"\n",
    "        ])\n",
    "        user = (\n",
    "            f\"\"\"\n",
    "            Extract and normalize Nobel-related named entities from the following articles.\n",
    "            Return a JSON array where each element has fields: 'people', 'organizations', 'prize'.\n",
    "            Each entry should contain unique canonical names.\n",
    "            Ensure the output is strictly valid JSON\\n\\n\n",
    "            \"Articles:\\n{joined}\n",
    "            \"\"\"\n",
    "        )\n",
    "        response = client.responses.create(\n",
    "            model=\"gpt-5\",\n",
    "            input=[{\"role\": \"user\", \"content\": user}],\n",
    "            # text_format=Entities\n",
    "        )\n",
    "        result = json.loads(response.output_text)\n",
    "        entities_list.extend(result)\n",
    "\n",
    "    return entities_list\n",
    "\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 5. Summarize",
   "id": "3480ffb6-a71b-4ea8-9931-ca670ea78289"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "input:cleaned_news.json\n",
    "<br/>output:summary(string/text)\n",
    "<br/>LLM API"
   ],
   "id": "963edd94-0da2-4488-a9f0-04ab54135c86"
  },
  {
   "cell_type": "code",
   "id": "848e9461-535c-4ff6-a7c6-68bdb501440d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-29T12:46:43.652712Z",
     "start_time": "2025-10-29T12:46:43.649645Z"
    }
   },
   "source": "\n",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "b5abd0f8-9e3b-4b01-9076-a23963b47e5d",
   "metadata": {},
   "source": [
    "## 6. Generate HTML Page\n",
    "include **a main summary**, **a list of key entities**, **a timeline of major developments**, and **links to the original source articles**\n",
    "\n",
    "input: summary,entities.json,timeline.json,cleaned_news.json(link)\n",
    "<br/>output:summary_page.html"
   ]
  },
  {
   "cell_type": "code",
   "id": "7ed36189-8088-42f8-8ac2-e814ac874c05",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-29T12:46:43.662110Z",
     "start_time": "2025-10-29T12:46:43.659072Z"
    }
   },
   "source": "\n",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "34d750ca-5c8f-42c7-87b0-7e929fe3e319",
   "metadata": {},
   "source": [
    "## 7. Run pipeline"
   ]
  },
  {
   "cell_type": "code",
   "id": "7be47e0a-5dea-45d2-a922-11a2afc17bb4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-01T11:55:16.882439Z",
     "start_time": "2025-11-01T11:55:16.879297Z"
    }
   },
   "source": [
    "# Input event name:\n",
    "keyword = \"Nobel Prize\" # 2025 Nobel Prize\n",
    "# Input start time:\n",
    "start_time = \"2025-09-23\" # “2025-09-23” \n",
    "# Input end time:\n",
    "end_time = \"2025-10-22\" # “2025-10-22”\n"
   ],
   "outputs": [],
   "execution_count": 3
  },
  {
   "cell_type": "code",
   "id": "7a344d13-4d9c-4135-b266-164ca2a6164f",
   "metadata": {},
   "source": [
    "# Crawl the news\n",
    "# crawl_the_news(keyword, start_time, end_time)\n",
    "# Clean the data\n",
    "# cleaned_the_data()\n",
    "\n",
    "# Extract information\n",
    "with open(\"cleanned_news.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    data = json.load(f)\n",
    "# Extract timeline\n",
    "timeline = extract_timeline(data)\n",
    "# save results\n",
    "json.dump(timeline, open(\"timeline.json\", \"w\", encoding=\"utf-8\"), ensure_ascii=False, indent=2)\n",
    "# Extract entities\n",
    "entities = extract_entities(data)\n",
    "# save results\n",
    "json.dump(entities, open(\"entities.json\", \"w\", encoding=\"utf-8\"), ensure_ascii=False, indent=2)\n",
    "\n",
    "# Summarize\n",
    "\n",
    "\n",
    "# Generate HTML page\n",
    "summary_page = \"\"\n",
    "\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "5fe19b84-c870-4700-b330-8764f0928ca4",
   "metadata": {},
   "source": [],
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
