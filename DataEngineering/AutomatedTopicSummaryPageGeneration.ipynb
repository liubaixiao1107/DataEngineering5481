{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "168b54c8-d579-4a5d-9a69-be25bd7d6991",
   "metadata": {},
   "source": [
    "# Automated Topic Summary Page Generation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d91b2bba-182e-41fd-ad76-e0358d5899a9",
   "metadata": {},
   "source": [
    "## 1. Project Introduction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6b8c5bf-2342-4fea-b12b-7110842776cb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fd2cd3af-46a6-4f58-9457-3819c09931de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libraries\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import time\n",
    "import random\n",
    "from datetime import datetime, timedelta, timezone\n",
    "from tqdm import tqdm\n",
    "import json\n",
    "import os\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6dbb7de-f840-4a2b-a339-568124b0652d",
   "metadata": {},
   "source": [
    "## 2. Crawl the news"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd89323f-f311-40ef-9a4d-371d870af621",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a5914515-caab-4f99-b531-c2369e1bb3e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Key\n",
    "NewsAPI_Key = \"8406ef98a8b24bec854801aa9f2c6a35\"\n",
    "GNews_Key = \"9a6066514e3ca31d8ec6c184b2c33594\"\n",
    "TheNewsAPI_Key = \"wEj2kyyJhPKLICmZavDq2MeJgbOr1KcyLbU0X3Au\"\n",
    "CurrentsAPI_Key = \"wMSLtPfn74YOMCOyIGv49vXAfIrD2bcXGVgEj_zN1AgA8b3G\"\n",
    "Mediastack_Key = \"465890a7953f6a540676c7c0fb86508a\"\n",
    "\n",
    "# URL\n",
    "NewsAPI_URL = \"https://newsapi.org/v2/everything\"\n",
    "GNews_URL = \"https://gnews.io/api/v4/search\"\n",
    "TheNewsAPI_URL = \"https://api.thenewsapi.com/v1/news/all\"\n",
    "Mediastack_URL = \"http://api.mediastack.com/v1/news\"\n",
    "\n",
    "# json name\n",
    "raw_json = \"raw_news.json\"\n",
    "cleaned_json = \"cleaned_news.json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b20c7d30-5920-4be9-a8f1-1c4dbb1984e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_article_content(url):\n",
    "    \"\"\"\n",
    "    Extract main content from news webpage URL\n",
    "    \"\"\"\n",
    "    try:\n",
    "        headers = {\n",
    "            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36'\n",
    "        }\n",
    "        \n",
    "        response = requests.get(url, headers=headers, timeout=15)\n",
    "        response.raise_for_status()\n",
    "        \n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "        \n",
    "        # Remove unwanted tags\n",
    "        for tag in ['script', 'style', 'nav', 'header', 'footer', 'aside']:\n",
    "            for element in soup.find_all(tag):\n",
    "                element.decompose()\n",
    "        \n",
    "        # Content selectors for news websites\n",
    "        content_selectors = [\n",
    "            # Main content area\n",
    "            'article',\n",
    "            'main',\n",
    "            '.main-content',\n",
    "            '.content-main',\n",
    "            '#main-content',\n",
    "            '#content-main',\n",
    "            \n",
    "            # News specific selector\n",
    "            '.article',\n",
    "            '.story',\n",
    "            '.news-article',\n",
    "            '.post',\n",
    "            '.entry',\n",
    "            \n",
    "            # Main content\n",
    "            '.article-body',\n",
    "            '.story-body',\n",
    "            '.post-body',\n",
    "            '.entry-content',\n",
    "            '.article-content',\n",
    "            '.story-content',\n",
    "            '.post-content',\n",
    "            '.news-content',\n",
    "            '.content-body',\n",
    "            '.body-content',\n",
    "            \n",
    "            # text content\n",
    "            '.text-content',\n",
    "            '.article-text',\n",
    "            '.story-text',\n",
    "            '.post-text',\n",
    "            \n",
    "            # General Content\n",
    "            '[class*=\"content\"]',\n",
    "            '[class*=\"article\"]',\n",
    "            '[class*=\"story\"]',\n",
    "            '[class*=\"post\"]',\n",
    "            '[class*=\"entry\"]',\n",
    "            '[class*=\"body\"]',\n",
    "            '[class*=\"text\"]',\n",
    "            \n",
    "            # Specific news websites\n",
    "            '.zn-body__paragraph',  # CNN\n",
    "            '.caas-body',           # Yahoo News\n",
    "            '.Article__Content',    # Bloomberg\n",
    "            '.article-section',     # Reuters\n",
    "            '.article-page',        # BBC\n",
    "            '.story-wrapper',       # NBC\n",
    "            '.article-wrapper',\n",
    "            \n",
    "            # Container selector\n",
    "            '.container',\n",
    "            '.wrapper',\n",
    "            '.main',\n",
    "            '#main',\n",
    "            '#content',\n",
    "            '.page-content'\n",
    "        ]\n",
    "        \n",
    "        # Try selectors first\n",
    "        for selector in content_selectors:\n",
    "            elements = soup.select(selector)\n",
    "            for element in elements:\n",
    "                text = element.get_text(strip=True)\n",
    "                text = re.sub(r'\\s+', ' ', text)\n",
    "                if len(text) > 200:\n",
    "                    return text\n",
    "        \n",
    "        # Fallback: combine paragraphs\n",
    "        paragraphs = soup.find_all('p')\n",
    "        if paragraphs:\n",
    "            content = ' '.join(p.get_text(strip=True) for p in paragraphs if len(p.get_text(strip=True)) > 50)\n",
    "            content = re.sub(r'\\s+', ' ', content)\n",
    "            if len(content) > 100:\n",
    "                return content\n",
    "        \n",
    "        return \"No valid content extracted\"\n",
    "        \n",
    "    except Exception as e:\n",
    "        return f\"Error: {str(e)}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98e4d161-af70-4d2e-b7d7-38625b788305",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_to_json(data, filename='raw_news.json'):\n",
    "    try:\n",
    "        with open(filename, 'w', encoding='utf-8') as f:\n",
    "            json.dump(data, f, ensure_ascii=False, indent=2)\n",
    "        print(f\"The data has been saved to {filename}\")\n",
    "        return True\n",
    "    except Exception as e:\n",
    "        print(f\"Error saving file: {e}\")\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e3816cf-77c5-4947-b820-c0020acec6fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_news_from_newsapi(keyword, start_time, end_time):\n",
    "    params = {\n",
    "        'q': keyword,\n",
    "        'from': start_time,\n",
    "        'to': end_time,\n",
    "        'sortBy': 'publishedAt',\n",
    "        'pageSize': 100,\n",
    "        'language': 'en',\n",
    "        'apiKey': NewsAPI_Key\n",
    "    }\n",
    "    \n",
    "    try:\n",
    "        response = requests.get(NewsAPI_URL, params=params)\n",
    "        response.raise_for_status()\n",
    "        data = response.json()\n",
    "        articles = data.get('articles', [])\n",
    "        print(f\"Fetched {len(articles)} articles from NewsAPI\")\n",
    "        return articles\n",
    "    except Exception as e:\n",
    "        print(f\"NewsAPI request failed: {e}\")\n",
    "        return []\n",
    "\n",
    "def fetch_news_from_gnews(keyword, start_time, end_time):  \n",
    "    params = {\n",
    "        'q': keyword,\n",
    "        'from': start_time,\n",
    "        'to': end_time,\n",
    "        'max': 100,\n",
    "        'lang': 'en',\n",
    "        'token': GNews_Key\n",
    "    }\n",
    "    \n",
    "    try:\n",
    "        response = requests.get(GNews_URL, params=params)\n",
    "        response.raise_for_status()\n",
    "        data = response.json()\n",
    "        articles = data.get('articles', [])\n",
    "        print(f\"Fetched {len(articles)} articles from GNews\")\n",
    "        return articles\n",
    "    except Exception as e:\n",
    "        print(f\"GNews API request failed: {e}\")\n",
    "        return []\n",
    "\n",
    "def fetch_news_from_thenewsapi(keyword, start_time, end_time):\n",
    "    params = {\n",
    "        'api_token': TheNewsAPI_Key,\n",
    "        'search': keyword,\n",
    "        'published_after': start_time,\n",
    "        'language': 'en',\n",
    "        'limit': 100\n",
    "    }\n",
    "    \n",
    "    try:\n",
    "        response = requests.get(TheNewsAPI_URL, params=params)\n",
    "        response.raise_for_status()\n",
    "        data = response.json()\n",
    "        articles = data.get('data', [])\n",
    "        print(f\"Fetched {len(articles)} articles from The News API\")\n",
    "        return articles\n",
    "    except Exception as e:\n",
    "        print(f\"The News API request failed: {e}\")\n",
    "        return []\n",
    "\n",
    "def fetch_nobel_news_from_currentsapi(keyword, start_time, end_time):\n",
    "\n",
    "    start_time = datetime.strptime(start_time, \"%Y-%m-%d\").replace(tzinfo=timezone.utc)\n",
    "    end_time = datetime.strptime(end_time, \"%Y-%m-%d\").replace(tzinfo=timezone.utc)\n",
    "    \n",
    "    url = (f'https://api.currentsapi.services/v1/search?'\n",
    "           f'keywords={keyword}&language=en&'\n",
    "           f'apiKey={CurrentsAPI_Key}&'\n",
    "           f'start_date{start_time}&end_date{end_time}')\n",
    "    \n",
    "    try:\n",
    "        response = requests.get(url)\n",
    "        response.raise_for_status()\n",
    "        \n",
    "        data = response.json()\n",
    "        \n",
    "        if data.get('status') == 'ok':\n",
    "            articles = data.get('news', [])\n",
    "            print(f\"Fetched {len(articles)} articles from CurrentsAPI\")\n",
    "            return articles\n",
    "        else:\n",
    "            print(f\"CurrentsAPI returned error: {data.get('message', 'Unknown error')}\")\n",
    "            return []\n",
    "        \n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"CurrentsAPI request failed: {e}\")\n",
    "        return []\n",
    "    except json.JSONDecodeError:\n",
    "        print(\"Failed to parse CurrentsAPI response\")\n",
    "        return []\n",
    "    except Exception as e:\n",
    "        print(f\"Unexpected error: {e}\")\n",
    "        return []\n",
    "\n",
    "def fetch_news_from_mediastack(keyword, start_time, end_time):\n",
    "    params = {\n",
    "        'access_key': Mediastack_Key,\n",
    "        'keywords': keyword,\n",
    "        'languages': 'en',\n",
    "        'limit': 100,\n",
    "        'sort': 'published_desc',\n",
    "        'date': f'{start_time},{end_time}'\n",
    "    }\n",
    "    \n",
    "    try:\n",
    "        response = requests.get(Mediastack_URL, params=params)\n",
    "        data = response.json()\n",
    "        \n",
    "        if 'data' in data:\n",
    "            articles = data.get('data', [])\n",
    "            print(f\"Fetched {len(articles)} articles from Mediastack\")\n",
    "            return articles\n",
    "        else:\n",
    "            print(f\"Error: {data.get('error', 'Unknown error')}\")\n",
    "            return []\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"Mediastack API error: {e}\")\n",
    "        return []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3a2c36e-2703-4030-ae85-fafa8c49a0e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_news_data(keyword, start_time, end_time):\n",
    "    print(\"Start obtaining news data...\")\n",
    "\n",
    "    raw_data = []\n",
    "    # \n",
    "    start_dt = datetime.strptime(start_time, \"%Y-%m-%d\")\n",
    "    end_dt = datetime.strptime(end_time, \"%Y-%m-%d\")\n",
    "    delta = end_dt - start_dt\n",
    "    random_days = random.randint(0, delta.days)\n",
    "    middle_time = (start_dt + timedelta(days=random_days)).strftime(\"%Y-%m-%d\")\n",
    "    \n",
    "    # Get data from all APIs\n",
    "    # Search twice\n",
    "    newsapi_articles_partone = fetch_news_from_newsapi(keyword, start_time, middle_time)\n",
    "    newsapi_articles_parttwo = fetch_news_from_newsapi(keyword, middle_time, end_time)\n",
    "    gnews_articles = fetch_news_from_gnews(keyword, start_time, end_time)\n",
    "    thenewsapi_articles = fetch_news_from_thenewsapi(keyword, start_time, end_time)\n",
    "    currents_articles = fetch_nobel_news_from_currentsapi(keyword, start_time, end_time)\n",
    "    # Search twice\n",
    "    mediastack_articles = fetch_news_from_mediastack(keyword, start_time, end_time)\n",
    "    \n",
    "    # Combine all articles\n",
    "    all_articles = []\n",
    "    all_articles.extend(newsapi_articles_partone)\n",
    "    all_articles.extend(newsapi_articles_parttwo)\n",
    "    all_articles.extend(gnews_articles)\n",
    "    all_articles.extend(thenewsapi_articles)\n",
    "    all_articles.extend(currents_articles)\n",
    "    all_articles.extend(mediastack_articles)\n",
    "    \n",
    "    print(f\"Total articles: {len(all_articles)}\")\n",
    "    \n",
    "    for i, article in enumerate(all_articles, 1):\n",
    "        print(f\"Processing {i}/{len(all_articles)}: {article['title'][:50]}...\")\n",
    "        \n",
    "        # Extract article content\n",
    "        text_content = extract_article_content(article['url'])\n",
    "        \n",
    "        # Build data structure\n",
    "        news_item = {\n",
    "            \"title\": article.get('title', 'No title'),\n",
    "            \"date\": article.get('publishedAt', 'No date'),\n",
    "            \"link\": article.get('url', ''),\n",
    "            \"text\": text_content\n",
    "        }\n",
    "        \n",
    "        raw_data.append(news_item)\n",
    "        \n",
    "        # Add delay to avoid rate limiting\n",
    "        time.sleep(1)\n",
    "    \n",
    "    return raw_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85dad55a-7778-4570-95c8-418d7b0acb48",
   "metadata": {},
   "outputs": [],
   "source": [
    "def crawl_the_news(keyword, start_time, end_time): \n",
    "    # Processing news data\n",
    "    raw_news_data = process_news_data(keyword, start_time, end_time)\n",
    "    \n",
    "    if raw_news_data:\n",
    "        # Save to JSON file\n",
    "        success = save_to_json(raw_news_data, raw_json)\n",
    "        \n",
    "        if success:\n",
    "            print(f\"Successfully processed {len(raw_news_data)} articles\")\n",
    "        else:\n",
    "            print(\"Failed to save file\")\n",
    "    else:\n",
    "        print(\"No data obtained\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff45c8cd-7fd1-446b-a5c2-dd74024238d1",
   "metadata": {},
   "source": [
    "## 3. Clean the data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55236166-3a35-457f-9d1d-191b873ca9c4",
   "metadata": {},
   "source": [
    "input: raw_news.json\n",
    "<br/>output: cleaned_news.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e0f617a-f528-44c0-a6fd-683f167ac9dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleaned_the_data(): \n",
    "    # Load the original file\n",
    "    with open(raw_json, \"r\", encoding=\"utf-8\") as f:\n",
    "        data = json.load(f)\n",
    "    \n",
    "    cleaned = []\n",
    "    seen = set()\n",
    "    \n",
    "    for item in data:\n",
    "        title = item.get(\"title\", \"\").strip()\n",
    "        link = item.get(\"link\", \"\").strip()\n",
    "        text = item.get(\"text\", \"\").strip()\n",
    "        date_str = item.get(\"date\", \"\").strip()\n",
    "    \n",
    "        # Skip empty records or invalid text\n",
    "        if not title or not link or not text:\n",
    "            continue\n",
    "        if text.lower() == \"no valid content extracted\".lower():\n",
    "            continue\n",
    "    \n",
    "        # Skip duplicates\n",
    "        if (title, link) in seen:\n",
    "            continue\n",
    "        seen.add((title, link))\n",
    "    \n",
    "        # Remove gibberish or control characters (keep printable English/Chinese chars)\n",
    "        def clean_str(s):\n",
    "            return re.sub(r\"[^\\x09\\x0A\\x0D\\x20-\\x7E\\u4E00-\\u9FFF]\", \" \", s)\n",
    "    \n",
    "        title = clean_str(title)\n",
    "        text = clean_str(text).lower()  # convert all text to lowercase\n",
    "    \n",
    "        # Normalize date format to YYYY-MM-DD\n",
    "        if date_str:\n",
    "            try:\n",
    "                dt = datetime.fromisoformat(date_str.replace(\"Z\", \"+00:00\"))\n",
    "                date_str = dt.strftime(\"%Y-%m-%d\")\n",
    "            except Exception:\n",
    "                match = re.search(r\"(\\d{4})[-/](\\d{2})[-/](\\d{2})\", date_str)\n",
    "                if match:\n",
    "                    date_str = \"-\".join(match.groups())\n",
    "                else:\n",
    "                    date_str = \"\"\n",
    "    \n",
    "        cleaned.append({\n",
    "            \"title\": title,\n",
    "            \"date\": date_str,\n",
    "            \"link\": link,\n",
    "            \"text\": text.strip()\n",
    "        })\n",
    "    \n",
    "    # Save cleaned data\n",
    "    with open(\"cleaned_news.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(cleaned, f, ensure_ascii=False, indent=2)\n",
    "    \n",
    "    print(f\"✅ Cleaning completed. {len(cleaned)} valid news articles saved to cleaned_news.json.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38a402ca-d967-4702-bd04-7b2f97e4c449",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cleaned data structure\n",
    "cleaned_data_list = []\n",
    "cleaned_data = {\n",
    "    \"title\" : title\n",
    "    \"date\" : date\n",
    "    \"link\" : link\n",
    "    \"text\" : text\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bdedd3a-573d-4c75-bf1b-4835b615facf",
   "metadata": {},
   "source": [
    "## 4. Extract the information"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "117fd8db-1b25-46fc-8664-cd4d862d7b19",
   "metadata": {},
   "source": [
    "input:cleaned_news.json\n",
    "<br/>output:timeline.json, entities.json\n",
    "<br/>NLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52a9ac94-e1aa-46c8-8688-3db2a62309fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# timeline structure\n",
    "timeline_list = []\n",
    "timeline = {\n",
    "    \"title\" : title\n",
    "    \"date\" : date\n",
    "}\n",
    "\n",
    "# entities.json\n",
    "entities_list = []\n",
    "\n",
    "entity = {\n",
    "    \"names\" : names\n",
    "    \"locations\" : locations\n",
    "    \"organizations\" organizaitions\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3480ffb6-a71b-4ea8-9931-ca670ea78289",
   "metadata": {},
   "source": [
    "## 5. Summarize"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "963edd94-0da2-4488-a9f0-04ab54135c86",
   "metadata": {},
   "source": [
    "input:cleaned_news.json\n",
    "<br/>output:summary(string/text)\n",
    "<br/>LLM API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "848e9461-535c-4ff6-a7c6-68bdb501440d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b5abd0f8-9e3b-4b01-9076-a23963b47e5d",
   "metadata": {},
   "source": [
    "## 6. Generate HTML Page\n",
    "include **a main summary**, **a list of key entities**, **a timeline of major developments**, and **links to the original source articles**\n",
    "\n",
    "input: summary,entities.json,timeline.json,cleaned_news.json(link)\n",
    "<br/>output:summary_page.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ed36189-8088-42f8-8ac2-e814ac874c05",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "34d750ca-5c8f-42c7-87b0-7e929fe3e319",
   "metadata": {},
   "source": [
    "## 7. Run pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7be47e0a-5dea-45d2-a922-11a2afc17bb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input event name:\n",
    "keyword = \"Nobel Prize\" # 2025 Nobel Prize\n",
    "# Input start time:\n",
    "start_time = \"2025-09-23\" # “2025-09-23” \n",
    "# Input end time:\n",
    "end_time = \"2025-10-22\" # “2025-10-22”\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a344d13-4d9c-4135-b266-164ca2a6164f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crawl the news\n",
    "# crawl_the_news(keyword, start_time, end_time)\n",
    "# Clean the data\n",
    "# cleaned_the_data()\n",
    "# Extract information\n",
    "\n",
    "# Summarize\n",
    "\n",
    "# Generate HTML page\n",
    "summary_page = \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fe19b84-c870-4700-b330-8764f0928ca4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
