{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "168b54c8-d579-4a5d-9a69-be25bd7d6991",
   "metadata": {},
   "source": [
    "# Automated Topic Summary Page Generation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d91b2bba-182e-41fd-ad76-e0358d5899a9",
   "metadata": {},
   "source": [
    "## 1. Project Introduction\n",
    "This project automates the extraction, summarization, and presentation of keyword-related news events. It consists of several modules:\n",
    "\n",
    "1. **Data Cleaning (`cleaned_the_data`)** – cleans and standardizes raw news articles from `raw_news.json`, removing invalid entries, deduplicating using (title, link) hashes, stripping non-printable characters, normalizing text and dates, and producing `cleaned_news.json` as a reliable dataset.\n",
    "\n",
    "2. **Timeline Extraction (`extract_timeline`)** – groups news articles by date and generates one-line summaries of major events per day, using GPT-5 with prompts focused on the target keyword, producing a structured timeline.\n",
    "\n",
    "3. **Entity Extraction (`extract_entities`)** – extracts and normalizes people, organizations, and entities mentioned in the news dataset, generating structured JSON for downstream reporting.\n",
    "\n",
    "4. **Automated Summarization (`summary`)** – generates a three-paragraph narrative summary using an authoritative timeline and optional headlines, enforcing strict formatting rules and avoiding any fabrication of facts.\n",
    "\n",
    "5. **Report Generation (`generate_report`)** – compiles the summary, entities, timeline, and links into a visually appealing HTML report (`index.html`) with interactive cards and a responsive layout, accompanied by a self-contained CSS file for styling.\n",
    "\n",
    "The pipeline produces clean, structured, and human-readable outputs, enabling efficient analysis, dissemination, and visualization of key events and entities related to the target keyword."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd2cd3af-46a6-4f58-9457-3819c09931de",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-01T12:01:51.260380Z",
     "start_time": "2025-11-01T12:01:49.931073Z"
    }
   },
   "outputs": [],
   "source": [
    "# import libraries\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import time\n",
    "import random\n",
    "from datetime import datetime, timedelta, timezone\n",
    "from tqdm import tqdm\n",
    "import json\n",
    "import os\n",
    "import re\n",
    "from collections import Counter, defaultdict\n",
    "from typing import List, Dict, Set,Any, Tuple\n",
    "from difflib import SequenceMatcher\n",
    "from openai import OpenAI\n",
    "import argparse, os, json, re, time, logging, threading, socket, random\n",
    "from urllib.parse import urlparse\n",
    "from requests.adapters import HTTPAdapter\n",
    "from urllib3.util.retry import Retry\n",
    "from pathlib import Path\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6dbb7de-f840-4a2b-a339-568124b0652d",
   "metadata": {},
   "source": [
    "## 2. Crawl the news\n",
    "**crawl_the_news** is the first stage of the pipeline, responsible for automatically collecting news data and exporting it as structured JSON. It integrates **five major news APIs (NewsAPI, GNews, TheNewsAPI, CurrentsAPI, Mediastack)** to maximize coverage and reduce information gaps. The workflow includes: randomly splitting the time range to improve NewsAPI retrieval; calling each API’s fetch_xxx() function to collect titles, timestamps, and URLs; extracting the full article text using BeautifulSoup with extensive CSS selectors and a fallback paragraph-based method; and normalizing all articles into a unified {title, date, link, text} structure. The final output is saved as raw_news.json, which serves as the input for cleaning, deduplication, entity extraction, and summarization modules."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5914515-caab-4f99-b531-c2369e1bb3e3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-29T12:46:43.522001Z",
     "start_time": "2025-10-29T12:46:43.487454Z"
    }
   },
   "outputs": [],
   "source": [
    "# Key\n",
    "NewsAPI_Key = \"8406ef98a8b24bec854801aa9f2c6a35\"\n",
    "GNews_Key = \"9a6066514e3ca31d8ec6c184b2c33594\"\n",
    "TheNewsAPI_Key = \"wEj2kyyJhPKLICmZavDq2MeJgbOr1KcyLbU0X3Au\"\n",
    "CurrentsAPI_Key = \"wMSLtPfn74YOMCOyIGv49vXAfIrD2bcXGVgEj_zN1AgA8b3G\"\n",
    "Mediastack_Key = \"465890a7953f6a540676c7c0fb86508a\"\n",
    "\n",
    "# URL\n",
    "NewsAPI_URL = \"https://newsapi.org/v2/everything\"\n",
    "GNews_URL = \"https://gnews.io/api/v4/search\"\n",
    "TheNewsAPI_URL = \"https://api.thenewsapi.com/v1/news/all\"\n",
    "Mediastack_URL = \"http://api.mediastack.com/v1/news\"\n",
    "\n",
    "# json name\n",
    "raw_json = \"raw_news.json\"\n",
    "cleaned_json = \"cleaned_news.json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b20c7d30-5920-4be9-a8f1-1c4dbb1984e8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-29T12:46:43.542235Z",
     "start_time": "2025-10-29T12:46:43.532869Z"
    }
   },
   "outputs": [],
   "source": [
    "def extract_article_content(url):\n",
    "    \"\"\"\n",
    "    Extract main content from news webpage URL\n",
    "    \"\"\"\n",
    "    try:\n",
    "        headers = {\n",
    "            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36'\n",
    "        }\n",
    "        \n",
    "        response = requests.get(url, headers=headers, timeout=15)\n",
    "        response.raise_for_status()\n",
    "        \n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "        \n",
    "        # Remove unwanted tags\n",
    "        for tag in ['script', 'style', 'nav', 'header', 'footer', 'aside']:\n",
    "            for element in soup.find_all(tag):\n",
    "                element.decompose()\n",
    "        \n",
    "        # Content selectors for news websites\n",
    "        content_selectors = [\n",
    "            # Main content area\n",
    "            'article',\n",
    "            'main',\n",
    "            '.main-content',\n",
    "            '.content-main',\n",
    "            '#main-content',\n",
    "            '#content-main',\n",
    "            \n",
    "            # News specific selector\n",
    "            '.article',\n",
    "            '.story',\n",
    "            '.news-article',\n",
    "            '.post',\n",
    "            '.entry',\n",
    "            \n",
    "            # Main content\n",
    "            '.article-body',\n",
    "            '.story-body',\n",
    "            '.post-body',\n",
    "            '.entry-content',\n",
    "            '.article-content',\n",
    "            '.story-content',\n",
    "            '.post-content',\n",
    "            '.news-content',\n",
    "            '.content-body',\n",
    "            '.body-content',\n",
    "            \n",
    "            # text content\n",
    "            '.text-content',\n",
    "            '.article-text',\n",
    "            '.story-text',\n",
    "            '.post-text',\n",
    "            \n",
    "            # General Content\n",
    "            '[class*=\"content\"]',\n",
    "            '[class*=\"article\"]',\n",
    "            '[class*=\"story\"]',\n",
    "            '[class*=\"post\"]',\n",
    "            '[class*=\"entry\"]',\n",
    "            '[class*=\"body\"]',\n",
    "            '[class*=\"text\"]',\n",
    "            \n",
    "            # Specific news websites\n",
    "            '.zn-body__paragraph',  # CNN\n",
    "            '.caas-body',           # Yahoo News\n",
    "            '.Article__Content',    # Bloomberg\n",
    "            '.article-section',     # Reuters\n",
    "            '.article-page',        # BBC\n",
    "            '.story-wrapper',       # NBC\n",
    "            '.article-wrapper',\n",
    "            \n",
    "            # Container selector\n",
    "            '.container',\n",
    "            '.wrapper',\n",
    "            '.main',\n",
    "            '#main',\n",
    "            '#content',\n",
    "            '.page-content'\n",
    "        ]\n",
    "        \n",
    "        # Try selectors first\n",
    "        for selector in content_selectors:\n",
    "            elements = soup.select(selector)\n",
    "            for element in elements:\n",
    "                text = element.get_text(strip=True)\n",
    "                text = re.sub(r'\\s+', ' ', text)\n",
    "                if len(text) > 200:\n",
    "                    return text\n",
    "        \n",
    "        # Fallback: combine paragraphs\n",
    "        paragraphs = soup.find_all('p')\n",
    "        if paragraphs:\n",
    "            content = ' '.join(p.get_text(strip=True) for p in paragraphs if len(p.get_text(strip=True)) > 50)\n",
    "            content = re.sub(r'\\s+', ' ', content)\n",
    "            if len(content) > 100:\n",
    "                return content\n",
    "        \n",
    "        return \"No valid content extracted\"\n",
    "        \n",
    "    except Exception as e:\n",
    "        return f\"Error: {str(e)}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98e4d161-af70-4d2e-b7d7-38625b788305",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-29T12:46:43.560421Z",
     "start_time": "2025-10-29T12:46:43.555132Z"
    }
   },
   "outputs": [],
   "source": [
    "def save_to_json(data, filename='raw_news.json'):\n",
    "    try:\n",
    "        with open(filename, 'w', encoding='utf-8') as f:\n",
    "            json.dump(data, f, ensure_ascii=False, indent=2)\n",
    "        print(f\"The data has been saved to {filename}\")\n",
    "        return True\n",
    "    except Exception as e:\n",
    "        print(f\"Error saving file: {e}\")\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e3816cf-77c5-4947-b820-c0020acec6fe",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-29T12:46:43.582278Z",
     "start_time": "2025-10-29T12:46:43.570473Z"
    }
   },
   "outputs": [],
   "source": [
    "def fetch_news_from_newsapi(keyword, start_time, end_time):\n",
    "    params = {\n",
    "        'q': keyword,\n",
    "        'from': start_time,\n",
    "        'to': end_time,\n",
    "        'sortBy': 'publishedAt',\n",
    "        'pageSize': 100,\n",
    "        'language': 'en',\n",
    "        'apiKey': NewsAPI_Key\n",
    "    }\n",
    "    \n",
    "    try:\n",
    "        response = requests.get(NewsAPI_URL, params=params)\n",
    "        response.raise_for_status()\n",
    "        data = response.json()\n",
    "        articles = data.get('articles', [])\n",
    "        print(f\"Fetched {len(articles)} articles from NewsAPI\")\n",
    "        return articles\n",
    "    except Exception as e:\n",
    "        print(f\"NewsAPI request failed: {e}\")\n",
    "        return []\n",
    "\n",
    "def fetch_news_from_gnews(keyword, start_time, end_time):  \n",
    "    params = {\n",
    "        'q': keyword,\n",
    "        'from': start_time,\n",
    "        'to': end_time,\n",
    "        'max': 100,\n",
    "        'lang': 'en',\n",
    "        'token': GNews_Key\n",
    "    }\n",
    "    \n",
    "    try:\n",
    "        response = requests.get(GNews_URL, params=params)\n",
    "        response.raise_for_status()\n",
    "        data = response.json()\n",
    "        articles = data.get('articles', [])\n",
    "        print(f\"Fetched {len(articles)} articles from GNews\")\n",
    "        return articles\n",
    "    except Exception as e:\n",
    "        print(f\"GNews API request failed: {e}\")\n",
    "        return []\n",
    "\n",
    "def fetch_news_from_thenewsapi(keyword, start_time, end_time):\n",
    "    params = {\n",
    "        'api_token': TheNewsAPI_Key,\n",
    "        'search': keyword,\n",
    "        'published_after': start_time,\n",
    "        'language': 'en',\n",
    "        'limit': 100\n",
    "    }\n",
    "    \n",
    "    try:\n",
    "        response = requests.get(TheNewsAPI_URL, params=params)\n",
    "        response.raise_for_status()\n",
    "        data = response.json()\n",
    "        articles = data.get('data', [])\n",
    "        print(f\"Fetched {len(articles)} articles from The News API\")\n",
    "        return articles\n",
    "    except Exception as e:\n",
    "        print(f\"The News API request failed: {e}\")\n",
    "        return []\n",
    "\n",
    "def fetch_nobel_news_from_currentsapi(keyword, start_time, end_time):\n",
    "\n",
    "    start_time = datetime.strptime(start_time, \"%Y-%m-%d\").replace(tzinfo=timezone.utc)\n",
    "    end_time = datetime.strptime(end_time, \"%Y-%m-%d\").replace(tzinfo=timezone.utc)\n",
    "    \n",
    "    url = (f'https://api.currentsapi.services/v1/search?'\n",
    "           f'keywords={keyword}&language=en&'\n",
    "           f'apiKey={CurrentsAPI_Key}&'\n",
    "           f'start_date{start_time}&end_date{end_time}')\n",
    "    \n",
    "    try:\n",
    "        response = requests.get(url)\n",
    "        response.raise_for_status()\n",
    "        \n",
    "        data = response.json()\n",
    "        \n",
    "        if data.get('status') == 'ok':\n",
    "            articles = data.get('news', [])\n",
    "            print(f\"Fetched {len(articles)} articles from CurrentsAPI\")\n",
    "            return articles\n",
    "        else:\n",
    "            print(f\"CurrentsAPI returned error: {data.get('message', 'Unknown error')}\")\n",
    "            return []\n",
    "        \n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"CurrentsAPI request failed: {e}\")\n",
    "        return []\n",
    "    except json.JSONDecodeError:\n",
    "        print(\"Failed to parse CurrentsAPI response\")\n",
    "        return []\n",
    "    except Exception as e:\n",
    "        print(f\"Unexpected error: {e}\")\n",
    "        return []\n",
    "\n",
    "def fetch_news_from_mediastack(keyword, start_time, end_time):\n",
    "    params = {\n",
    "        'access_key': Mediastack_Key,\n",
    "        'keywords': keyword,\n",
    "        'languages': 'en',\n",
    "        'limit': 100,\n",
    "        'sort': 'published_desc',\n",
    "        'date': f'{start_time},{end_time}'\n",
    "    }\n",
    "    \n",
    "    try:\n",
    "        response = requests.get(Mediastack_URL, params=params)\n",
    "        data = response.json()\n",
    "        \n",
    "        if 'data' in data:\n",
    "            articles = data.get('data', [])\n",
    "            print(f\"Fetched {len(articles)} articles from Mediastack\")\n",
    "            return articles\n",
    "        else:\n",
    "            print(f\"Error: {data.get('error', 'Unknown error')}\")\n",
    "            return []\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"Mediastack API error: {e}\")\n",
    "        return []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3a2c36e-2703-4030-ae85-fafa8c49a0e9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-29T12:46:43.598341Z",
     "start_time": "2025-10-29T12:46:43.591052Z"
    }
   },
   "outputs": [],
   "source": [
    "def process_news_data(keyword, start_time, end_time):\n",
    "    print(\"Start obtaining news data...\")\n",
    "\n",
    "    raw_data = []\n",
    "    # \n",
    "    start_dt = datetime.strptime(start_time, \"%Y-%m-%d\")\n",
    "    end_dt = datetime.strptime(end_time, \"%Y-%m-%d\")\n",
    "    delta = end_dt - start_dt\n",
    "    random_days = random.randint(0, delta.days)\n",
    "    middle_time = (start_dt + timedelta(days=random_days)).strftime(\"%Y-%m-%d\")\n",
    "    \n",
    "    # Get data from all APIs\n",
    "    # Search twice\n",
    "    newsapi_articles_partone = fetch_news_from_newsapi(keyword, start_time, middle_time)\n",
    "    newsapi_articles_parttwo = fetch_news_from_newsapi(keyword, middle_time, end_time)\n",
    "    gnews_articles = fetch_news_from_gnews(keyword, start_time, end_time)\n",
    "    thenewsapi_articles = fetch_news_from_thenewsapi(keyword, start_time, end_time)\n",
    "    currents_articles = fetch_nobel_news_from_currentsapi(keyword, start_time, end_time)\n",
    "    # Search twice\n",
    "    mediastack_articles = fetch_news_from_mediastack(keyword, start_time, end_time)\n",
    "    \n",
    "    # Combine all articles\n",
    "    all_articles = []\n",
    "    all_articles.extend(newsapi_articles_partone)\n",
    "    all_articles.extend(newsapi_articles_parttwo)\n",
    "    all_articles.extend(gnews_articles)\n",
    "    all_articles.extend(thenewsapi_articles)\n",
    "    all_articles.extend(currents_articles)\n",
    "    all_articles.extend(mediastack_articles)\n",
    "    \n",
    "    print(f\"Total articles: {len(all_articles)}\")\n",
    "    \n",
    "    for i, article in enumerate(all_articles, 1):\n",
    "        print(f\"Processing {i}/{len(all_articles)}: {article['title'][:50]}...\")\n",
    "        \n",
    "        # Extract article content\n",
    "        text_content = extract_article_content(article['url'])\n",
    "        \n",
    "        # Build data structure\n",
    "        news_item = {\n",
    "            \"title\": article.get('title', 'No title'),\n",
    "            \"date\": article.get('publishedAt', 'No date'),\n",
    "            \"link\": article.get('url', ''),\n",
    "            \"text\": text_content\n",
    "        }\n",
    "        \n",
    "        raw_data.append(news_item)\n",
    "        \n",
    "        # Add delay to avoid rate limiting\n",
    "        time.sleep(1)\n",
    "    \n",
    "    return raw_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85dad55a-7778-4570-95c8-418d7b0acb48",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-29T12:46:43.612339Z",
     "start_time": "2025-10-29T12:46:43.607241Z"
    }
   },
   "outputs": [],
   "source": [
    "def crawl_the_news(keyword, start_time, end_time): \n",
    "    # Processing news data\n",
    "    raw_news_data = process_news_data(keyword, start_time, end_time)\n",
    "    \n",
    "    if raw_news_data:\n",
    "        # Save to JSON file\n",
    "        success = save_to_json(raw_news_data, raw_json)\n",
    "        \n",
    "        if success:\n",
    "            print(f\"Successfully processed {len(raw_news_data)} articles\")\n",
    "        else:\n",
    "            print(\"Failed to save file\")\n",
    "    else:\n",
    "        print(\"No data obtained\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff45c8cd-7fd1-446b-a5c2-dd74024238d1",
   "metadata": {},
   "source": [
    "## 3. Clean the data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55236166-3a35-457f-9d1d-191b873ca9c4",
   "metadata": {},
   "source": [
    "**cleaned_the_data** cleans and standardizes the raw dataset. It takes raw_news.json as input and produces a filtered, deduplicated, and normalized dataset cleaned_news.json. The module removes empty or invalid articles, eliminates duplicates using a (title, link) hash set, strips non-printable characters, converts article text to lowercase, and normalizes all dates to YYYY-MM-DD. Each cleaned record is stored in a unified structure {title, date, link, text}. The resulting cleaned_news.json serves as the clean and reliable input for downstream summarization, entity extraction, and timeline generation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e0f617a-f528-44c0-a6fd-683f167ac9dd",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-29T12:46:43.629138Z",
     "start_time": "2025-10-29T12:46:43.621230Z"
    }
   },
   "outputs": [],
   "source": [
    "# cleaned data structure:\n",
    "\n",
    "# cleaned_data_list = []\n",
    "# cleaned_data = {\n",
    "#     \"title\" : title\n",
    "#     \"date\" : date\n",
    "#     \"link\" : link\n",
    "#     \"text\" : text\n",
    "# }\n",
    "\n",
    "def cleaned_the_data():\n",
    "    # Load the original file\n",
    "    with open(raw_json, \"r\", encoding=\"utf-8\") as f:\n",
    "        data = json.load(f)\n",
    "    \n",
    "    cleaned = []\n",
    "    seen = set()\n",
    "    \n",
    "    for item in data:\n",
    "        title = item.get(\"title\", \"\").strip()\n",
    "        link = item.get(\"link\", \"\").strip()\n",
    "        text = item.get(\"text\", \"\").strip()\n",
    "        date_str = item.get(\"date\", \"\").strip()\n",
    "    \n",
    "        # Skip empty records or invalid text\n",
    "        if not title or not link or not text:\n",
    "            continue\n",
    "        if text.lower() == \"no valid content extracted\".lower():\n",
    "            continue\n",
    "    \n",
    "        # Skip duplicates\n",
    "        if (title, link) in seen:\n",
    "            continue\n",
    "        seen.add((title, link))\n",
    "    \n",
    "        # Remove gibberish or control characters (keep printable English/Chinese chars)\n",
    "        def clean_str(s):\n",
    "            return re.sub(r\"[^\\x09\\x0A\\x0D\\x20-\\x7E\\u4E00-\\u9FFF]\", \" \", s)\n",
    "    \n",
    "        title = clean_str(title)\n",
    "        text = clean_str(text).lower()  # convert all text to lowercase\n",
    "    \n",
    "        # Normalize date format to YYYY-MM-DD\n",
    "        if date_str:\n",
    "            try:\n",
    "                dt = datetime.fromisoformat(date_str.replace(\"Z\", \"+00:00\"))\n",
    "                date_str = dt.strftime(\"%Y-%m-%d\")\n",
    "            except Exception:\n",
    "                match = re.search(r\"(\\d{4})[-/](\\d{2})[-/](\\d{2})\", date_str)\n",
    "                if match:\n",
    "                    date_str = \"-\".join(match.groups())\n",
    "                else:\n",
    "                    date_str = \"\"\n",
    "    \n",
    "        cleaned.append({\n",
    "            \"title\": title,\n",
    "            \"date\": date_str,\n",
    "            \"link\": link,\n",
    "            \"text\": text.strip()\n",
    "        })\n",
    "    \n",
    "    # Save cleaned data\n",
    "    with open(\"../../assignment2/cleaned_news.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(cleaned, f, ensure_ascii=False, indent=2)\n",
    "    \n",
    "    print(f\"Cleaning completed. {len(cleaned)} valid news articles saved to cleaned_news.json.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bdedd3a-573d-4c75-bf1b-4835b615facf",
   "metadata": {},
   "source": [
    "## 4. Extract the information"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "117fd8db-1b25-46fc-8664-cd4d862d7b19",
   "metadata": {},
   "source": [
    "### extract_timeline \n",
    "generates a structured daily event timeline using GPT-5.\n",
    "It takes the cleaned dataset as input, groups all articles by date, and composes a compact text block for each day.\n",
    "A date-specific prompt is then sent to GPT-5, instructing the model to ignore all items unrelated to the specified keyword and produce exactly one sentence describing the key event of that day, including the main people, organizations, and time.\n",
    "Each returned summary is stored in the unified structure {date, event}.\n",
    "The resulting timeline list provides a concise, date-ordered narrative foundation for the final summary webpage.\n",
    "\n",
    "### extract_entities\n",
    "extract_entities extracts and normalizes all keyword-related entities from the cleaned dataset using GPT-5.\n",
    "For each article, a prompt is sent instructing the model to identify and canonicalize names of people, organizations, and prizes, and return them in strict JSON format.\n",
    "The module parses each JSON response and merges all extracted entries into a unified list following the structure {people, organizations, prize}.\n",
    "The resulting entity set forms the structured knowledge base used for the final summary page’s key-entity section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52a9ac94-e1aa-46c8-8688-3db2a62309fc",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-01T11:56:15.667947Z",
     "start_time": "2025-11-01T11:56:15.667640Z"
    }
   },
   "outputs": [],
   "source": [
    "# timeline structure\n",
    "\n",
    "# timeline_list = []\n",
    "# timeline = {\n",
    "#     \"date\" : date\n",
    "#     \"event\" : event\n",
    "# }\n",
    "\n",
    "# entities.json\n",
    "\n",
    "# entities_list = []\n",
    "# entity = {\n",
    "#     \"people\" : people\n",
    "#     \"prize\" : prize\n",
    "#     \"organizations\" organizaitions\n",
    "# }\n",
    "\n",
    "OPENAI_API_KEY=\"test\"\n",
    "\n",
    "client = OpenAI(api_key=OPENAI_API_KEY)\n",
    "\n",
    "# ---------------------------------------------------------------------\n",
    "# 1. Extract timeline\n",
    "# ---------------------------------------------------------------------\n",
    "def extract_timeline(news,keyword):\n",
    "    \"\"\"\n",
    "    Group articles by date and ask GPT-5 to summarize each day's major event.\n",
    "    Returns:\n",
    "        timeline_list = [\n",
    "            {\"date\": \"YYYY-MM-DD\", \"event\": \"...\"},\n",
    "            ...\n",
    "        ]\n",
    "    \"\"\"\n",
    "    # group by date\n",
    "    by_date = defaultdict(list)\n",
    "    for item in news:\n",
    "        d = item.get(\"date\")\n",
    "        if d:\n",
    "            by_date[d].append(item)\n",
    "\n",
    "    timeline_list = []\n",
    "    # process each date, oldest to newest,come up with one-line summary\n",
    "    for d, items in sorted(by_date.items(), key=lambda x: x[0]):\n",
    "        # prepare short text for the model\n",
    "        joined = \"\\n\\n\".join([\n",
    "            f\"- Title: {it.get('title','')}\\n  Text: {it.get('text','')}\"\n",
    "            for it in items\n",
    "        ])\n",
    "\n",
    "        system = (\n",
    "            f\"\"\"\n",
    "            You are an expert summarizer. Based only on the provided file content, without searching the web,\n",
    "            remove all news items that are not directly related to the {keyword}\n",
    "            For each remaining Nobel-related event, compress it into exactly one sentence.\n",
    "            Each sentence must clearly state the key person(s), organization(s), and time.\n",
    "            Return the final set of one-sentence events only.\n",
    "            \"\"\"\n",
    "            # \"\"\"\n",
    "            # You are given several text messages that were recorded on the same day.\n",
    "            # Your task is to identify and extract the distinct events mentioned in these messages.\n",
    "            # \"\"\"\n",
    "        )\n",
    "        user = f\"Date: {d}\\nNews snippets:\\n{joined}\\n\\nReturn only the final one-line event.\"\n",
    "\n",
    "        response = client.responses.create(\n",
    "            model=\"gpt-5\",\n",
    "            input=[\n",
    "                {\"role\": \"system\", \"content\": system},\n",
    "                {\"role\": \"user\", \"content\": user},\n",
    "            ]\n",
    "        )\n",
    "        content=response.output_text\n",
    "        timeline_list.append({\"date\": d, \"event\": content})\n",
    "\n",
    "    return timeline_list\n",
    "\n",
    "\n",
    "# ---------------------------------------------------------------------\n",
    "# 2. Extract entities\n",
    "# ---------------------------------------------------------------------\n",
    "\n",
    "def extract_entities(news,keyword):\n",
    "    \"\"\"\n",
    "    Ask GPT-5 to extract and normalize people, organizations, and prizes\n",
    "    from the entire dataset.\n",
    "    Returns:\n",
    "        entities_dict = {\n",
    "            \"people\": [...],\n",
    "            \"organizations\": [...],\n",
    "            \"prize\": [...]\n",
    "        }\n",
    "    \"\"\"\n",
    "\n",
    "    entities_list = []\n",
    "    for item in news:\n",
    "        # concat limited sample of texts for prompt (avoid overly long input)\n",
    "        joined = \"\\n\\n\".join([\n",
    "            f\"- Title: {item.get('title','')}\\n  Text: {item.get('text','')}\"\n",
    "        ])\n",
    "        user = (\n",
    "            f\"\"\"\n",
    "            Extract and normalize{keyword}-related named entities from the following articles.\n",
    "            Return a JSON array where each element has fields: 'people', 'organizations', 'prize'.\n",
    "            Each entry should contain unique canonical names.\n",
    "            Ensure the output is strictly valid JSON\\n\\n\n",
    "            \"Articles:\\n{joined}\n",
    "            \"\"\"\n",
    "        )\n",
    "        response = client.responses.create(\n",
    "            model=\"gpt-5\",\n",
    "            input=[{\"role\": \"user\", \"content\": user}],\n",
    "            # text_format=Entities\n",
    "        )\n",
    "        result = json.loads(response.output_text)\n",
    "        entities_list.extend(result)\n",
    "\n",
    "    return entities_list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3480ffb6-a71b-4ea8-9931-ca670ea78289",
   "metadata": {},
   "source": [
    "## 5. Summarize"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "963edd94-0da2-4488-a9f0-04ab54135c86",
   "metadata": {},
   "source": [
    "**summary()** generates a three-paragraph English narrative summary of the 2025 Nobel Prizes. It takes cleaned_news.json as input and optionally uses timeline.json as the authoritative timeline, producing final_summary.txt in the specified output directory. The function filters news for keyword-related items, constructs system and user prompts, and calls the API https://open.bigmodel.cn/api/paas/v4/chat/completions via call_glm(). The system prompt (OVERALL_PROMPT) enforces strict format, style, and no-fabrication rules, while the user prompt provides the timeline and news headlines, instructing the model to output exactly three paragraphs. The response is processed to ensure three paragraphs; extra paragraphs are truncated and fewer paragraphs are refactored using refactor_to_three_paragraphs(). AdaptiveLimiter manages request rate and handles 429, timeout, or other errors. The final output aligns with the timeline and ignores non-Nobel or non-2025 events, providing high-quality text for downstream analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "848e9461-535c-4ff6-a7c6-68bdb501440d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-29T12:46:43.652712Z",
     "start_time": "2025-10-29T12:46:43.649645Z"
    }
   },
   "outputs": [],
   "source": [
    "API_URL = \"https://open.bigmodel.cn/api/paas/v4/chat/completions\"\n",
    "\n",
    "logging.basicConfig(level=logging.INFO,\n",
    "    format=\"%(asctime)s | %(levelname)s | %(message)s\", datefmt=\"%H:%M:%S\")\n",
    "log = logging.getLogger(\"topic2\")\n",
    "\n",
    "def ensure_dir(p): os.makedirs(p, exist_ok=True); return p\n",
    "def load_json(p):\n",
    "    with open(p, \"r\", encoding=\"utf-8\") as f: return json.load(f)\n",
    "def domain_of(url: str) -> str:\n",
    "    try: return urlparse(url).netloc or \"\"\n",
    "    except: return \"\"\n",
    "def is_refusal(text: str) -> bool:\n",
    "    if not text: return True\n",
    "\n",
    "\n",
    "_SENT_SPLIT = re.compile(r'(?<=[。！？!?\\.])\\s+(?=[A-Z“\"(\\[]|[A-Z][a-z])')\n",
    "def split_sentences(text: str):\n",
    "    t = re.sub(r'\\s+', ' ', (text or \"\").strip())\n",
    "    parts = _SENT_SPLIT.split(t)\n",
    "    if len(parts) <= 1:\n",
    "        parts = re.split(r'(?<=[\\.!?])\\s+', t)\n",
    "    return [s.strip() for s in parts if s.strip()]\n",
    "\n",
    "def refactor_to_three_paragraphs(text: str):\n",
    "    sents = split_sentences(text)\n",
    "    if not sents: return (text or \"\").strip()\n",
    "    n = len(sents)\n",
    "    if n <= 3:\n",
    "        p1 = \" \".join(sents[:1]); p2 = \" \".join(sents[1:2]); p3 = \" \".join(sents[2:])\n",
    "        return \"\\n\\n\".join([p for p in (p1,p2,p3) if p]).strip()\n",
    "    p1_len = min(5, max(3, n//6 or 3))\n",
    "    rem = n - p1_len\n",
    "    p2_len = min(10, max(6, rem//2 or 6))\n",
    "    p3_len = n - p1_len - p2_len\n",
    "    if p3_len < 3 and n >= 12:\n",
    "        move = min(3 - p3_len, p2_len - 6)\n",
    "        if move > 0:\n",
    "            p2_len -= move\n",
    "            p3_len += move\n",
    "    p1 = \" \".join(sents[:p1_len]).strip()\n",
    "    p2 = \" \".join(sents[p1_len:p1_len+p2_len]).strip()\n",
    "    p3 = \" \".join(sents[p1_len+p2_len:]).strip()\n",
    "    return \"\\n\\n\".join([p for p in (p1,p2,p3) if p]).strip()\n",
    "\n",
    "_NOBEL_PAT = re.compile(r\"\\bNobel\\b|\", re.IGNORECASE)\n",
    "_YEAR_2025_PAT = re.compile(r\"\\b2025\\b|2025\")\n",
    "\n",
    "def is_nobel_related(title: str, text: str = \"\") -> bool:\n",
    "    t = (title or \"\").strip()\n",
    "    if not t: return False\n",
    "    return bool(_NOBEL_PAT.search(t) or _NOBEL_PAT.search(text or \"\"))\n",
    "\n",
    "def is_year_2025(title: str, date_str: str, text: str = \"\") -> bool:\n",
    "    if _YEAR_2025_PAT.search(title or \"\") or _YEAR_2025_PAT.search(text or \"\"):\n",
    "        return True\n",
    "    if (date_str or \"\").startswith(\"2025-\"):\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "class AdaptiveLimiter:\n",
    "    def __init__(self, qps: float = 0.5, min_qps: float = 0.15, max_qps: float = 1.2):\n",
    "        self.lock = threading.Lock(); self.qps=qps; self.min_qps=min_qps; self.max_qps=max_qps\n",
    "        self.last = 0.0; self.cool_until = 0.0\n",
    "    def wait(self):\n",
    "        with self.lock:\n",
    "            now = time.monotonic()\n",
    "            if now < self.cool_until:\n",
    "                time.sleep(self.cool_until - now); now = time.monotonic()\n",
    "            interval = 1.0 / max(self.qps, self.min_qps)\n",
    "            delta = interval - (now - self.last)\n",
    "            if delta > 0: time.sleep(delta); now = time.monotonic()\n",
    "            self.last = now\n",
    "    def punish_429(self):\n",
    "        with self.lock:\n",
    "            self.qps = max(self.min_qps, self.qps * 0.6)\n",
    "            cool = 6.0 + random.random()*6.0\n",
    "            self.cool_until = time.monotonic() + cool\n",
    "            log.warning(f\"set off 429：slowdown {self.qps:.2f} QPS，and freeze {cool:.1f}s\")\n",
    "    def punish_timeout(self):\n",
    "        with self.lock:\n",
    "            self.qps = max(self.min_qps, self.qps * 0.8)\n",
    "            log.warning(f\"quest over time ：slowdown {self.qps:.2f} QPS\")\n",
    "\n",
    "def make_session():\n",
    "    s = requests.Session()\n",
    "    retry = Retry(\n",
    "        total=3, connect=3, read=3,\n",
    "        backoff_factor=1.2,\n",
    "        status_forcelist=[429, 500, 502, 503, 504],\n",
    "        allowed_methods=[\"POST\"]\n",
    "    )\n",
    "    adapter = HTTPAdapter(max_retries=retry, pool_maxsize=2)\n",
    "    s.mount(\"https://\", adapter); s.mount(\"http://\", adapter)\n",
    "    s.headers.update({\"Accept\":\"application/json\", \"Connection\":\"close\"})\n",
    "    return s\n",
    "\n",
    "def call_glm(session: requests.Session, limiter: AdaptiveLimiter, api_key: str, messages,\n",
    "             model=\"glm-4.5-flash\", temperature=0.26, max_tokens=1400,\n",
    "             timeout=120, max_retries=3):\n",
    "    headers = {\"Authorization\": f\"Bearer {api_key}\", \"Content-Type\": \"application/json\"}\n",
    "    payload = {\"model\": model, \"messages\": messages, \"temperature\": temperature,\n",
    "               \"max_tokens\": max_tokens, \"stream\": False}\n",
    "    last_err = None\n",
    "    for attempt in range(max_retries + 1):\n",
    "        limiter.wait()\n",
    "        try:\n",
    "            resp = session.post(API_URL, headers=headers, json=payload, timeout=timeout)\n",
    "            if resp.status_code == 200:\n",
    "                j = resp.json()\n",
    "                txt = j.get(\"choices\", [{}])[0].get(\"message\", {}).get(\"content\", \"\")\n",
    "                return txt\n",
    "            if resp.status_code == 429:\n",
    "                limiter.punish_429()\n",
    "                time.sleep(1.2 + random.random()); continue\n",
    "            if resp.status_code in (408, 500, 502, 503, 504):\n",
    "                wait = (1.8 + random.random()) * (2 ** attempt)\n",
    "                log.warning(f\"default {resp.status_code}：{resp.text[:160]}...，{wait:.1f}s retry\")\n",
    "                time.sleep(wait); continue\n",
    "            raise RuntimeError(f\"HTTP {resp.status_code}: {resp.text[:500]}\")\n",
    "        except (requests.Timeout, socket.timeout) as e:\n",
    "            last_err = e; limiter.punish_timeout()\n",
    "            if attempt < max_retries:\n",
    "                wait = (1.2 + random.random()) * (2 ** attempt)\n",
    "                log.warning(f\"quest over time，{wait:.1f}s retry\")\n",
    "                time.sleep(wait); continue\n",
    "            break\n",
    "        except requests.RequestException as e:\n",
    "            last_err = e; break\n",
    "    raise RuntimeError(f\"model unusable：{last_err or 'unknown error'}\")\n",
    "\n",
    "# ---------------- main ----------------\n",
    "def summary(keyword):\n",
    "    #prompts\n",
    "    TIMELINE_PREAMBLE = (\n",
    "        \"Here is an authoritative timeline (date + event) that MUST be treated as ground truth. \"\n",
    "        \"When headlines conflict, resolve in favor of the timeline. \"\n",
    "        \"Do NOT invent prizewinners or entities not supported by the timeline/headlines.\"\n",
    "    )\n",
    "    \n",
    "    OVERALL_PROMPT = f\"\"\"\n",
    "        You will receive:\n",
    "        1) An authoritative timeline of the {keyword} (ground truth).\n",
    "        2) Optionally, a list of headlines (title, publisher domain, date).\n",
    "        \n",
    "        Your task: write a **single English narrative summary** ONLY about the **{keyword}**.\n",
    "        \n",
    "        STRICT FORMAT & STYLE:\n",
    "        • **Output EXACTLY THREE PARAGRAPHS**, with a blank line between paragraphs.\n",
    "        • Paragraph 1 (3–5 sentences): concise highlights in flowing prose — e.g.,\n",
    "          “the {keyword} honored key contributors, highlighting their impact in relevant fields.\n",
    "           Use such phrasing **only if these facts are supported**; otherwise use generic wording without adding specifics.”\n",
    "        • Paragraph 2 (~200–300 words): an integrative overview linking breakthroughs and societal meaning.\n",
    "        • Paragraph 3 (~180–260 words): synthesize 3–5 cross-cutting themes with transitions (meanwhile, in turn, as a result, by contrast…).\n",
    "          Do **not** enumerate by date or outlet.\n",
    "        \n",
    "        HARD CONSTRAINTS:\n",
    "        • **NO FABRICATION**: do not invent winners, categories, dates, affiliations, numbers, or methods.\n",
    "        • **TIMELINE-ALIGNED**: if any conflict arises, prefer the timeline; otherwise generalize.\n",
    "        • **FOCUS**: Ignore any non-{keyword} or non-2025 items entirely.\n",
    "        • Output plain text only (no JSON, no headers, no lists).\n",
    "        \"\"\"\n",
    "\n",
    "    ap = argparse.ArgumentParser()\n",
    "    ap.add_argument(\"--input\", default=\"cleaned_news.json\")\n",
    "    ap.add_argument(\"--timeline\", default=\"timeline.json\")\n",
    "    ap.add_argument(\"--output_dir\", default=\"outputs\")\n",
    "    ap.add_argument(\"--model\", default=\"glm-4.5-flash\")\n",
    "    ap.add_argument(\"--api-key\", default=\"d0b8bc52cf6b4c368982dfdd32384757.UcWBjZr72H7AWgyN\")\n",
    "    ap.add_argument(\"--qps\", type=float, default=0.5)\n",
    "    ap.add_argument(\"--use-headlines\", action=\"store_true\")\n",
    "    args = ap.parse_args()\n",
    "\n",
    "    ensure_dir(args.output_dir)\n",
    "\n",
    "    # load timeline\n",
    "    if not args.timeline or not os.path.exists(args.timeline):\n",
    "        log.warning(\"can not find timeline ，try headlines\")\n",
    "        timeline = []\n",
    "    else:\n",
    "        timeline = load_json(args.timeline)\n",
    "        if not isinstance(timeline, list):\n",
    "            log.warning(\" \"); timeline = []\n",
    "        else:\n",
    "            log.info(f\"loaded timeline：{args.timeline}（{len(timeline)} ）\")\n",
    "    # load news\n",
    "    if not os.path.exists(args.input):\n",
    "        log.error(f\"none input file：{args.input}\"); return\n",
    "    data = load_json(args.input)\n",
    "    log.info(f\"load data：{args.input}，total：{len(data)}\")\n",
    "\n",
    "    nobel_items = []\n",
    "    for it in data:\n",
    "        title = (it.get(\"title\") or \"\").strip()\n",
    "        text  = (it.get(\"text\")  or \"\")\n",
    "        date  = (it.get(\"date\")  or \"unknown\").strip() or \"unknown\"\n",
    "        if not title: continue\n",
    "        if is_nobel_related(title, text) and is_year_2025(title, date, text):\n",
    "            nobel_items.append({\"date\": date, \"title\": title, \"source\": domain_of(it.get(\"link\",\"\") or \"\")})\n",
    "\n",
    "    if not timeline and not nobel_items:\n",
    "        out = os.path.join(args.output_dir, \"final_summary.txt\")\n",
    "        with open(out, \"w\", encoding=\"utf-8\") as f:\n",
    "            f.write(f\"No{keyword}–related timeline or headlines were found.\")\n",
    "        log.info(f\"written：{out}\"); return\n",
    "\n",
    "    system_prompt = OVERALL_PROMPT\n",
    "    blocks = []\n",
    "\n",
    "    if timeline:\n",
    "        blocks.append(\"TIMELINE (authoritative):\\n\" + json.dumps(timeline, ensure_ascii=False, indent=2))\n",
    "\n",
    "    if args.use_headlines and nobel_items:\n",
    "        items_blob = json.dumps({\"items\": sorted(nobel_items, key=lambda x: x['date'])}, ensure_ascii=False)\n",
    "        blocks.append(\"HEADLINES (secondary evidence):\\n\" + items_blob)\n",
    "\n",
    "    user_msg = (\n",
    "        \"Use the timeline as ground truth. If any conflict arises, prefer the timeline.\\n\\n\"\n",
    "        + (\"\\n\\n\".join(blocks) if blocks else \"No timeline provided; rely on headlines without fabrication.\")\n",
    "        + \"\\n\\nRemember: Output EXACTLY THREE PARAGRAPHS separated by a blank line. \"\n",
    "          \"Do not list dates or outlets; write flowing prose.\"\n",
    "    )\n",
    "\n",
    "    session = make_session()\n",
    "    limiter = AdaptiveLimiter(qps=args.qps)\n",
    "\n",
    "    try:\n",
    "        raw = call_glm(session, limiter, args.api_key or os.getenv(\"ZHIPU_API_KEY\",\"\"),\n",
    "                       [{\"role\":\"system\",\"content\":system_prompt},\n",
    "                        {\"role\":\"user\",\"content\":user_msg}],\n",
    "                       model=args.model, temperature=0.26, max_tokens=2400, timeout=120)\n",
    "    except Exception as e:\n",
    "        log.warning(f\"generate fail：{e}\")\n",
    "        raw = \"\"\n",
    "\n",
    "    if is_refusal(raw):\n",
    "        final = (\"could not generate\")\n",
    "    else:\n",
    "        paras = [p for p in re.split(r'\\n\\s*\\n', raw.strip()) if p.strip()]\n",
    "        final = (\"\\n\\n\".join(re.sub(r'\\s+',' ', p).strip() for p in paras[:3])\n",
    "                 if len(paras) >= 3 else refactor_to_three_paragraphs(raw))\n",
    "\n",
    "    out_path = os.path.join(args.output_dir, \"final_summary.txt\")\n",
    "    with open(out_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(final)\n",
    "    log.info(f\"written：{out_path}\\n finish\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5abd0f8-9e3b-4b01-9076-a23963b47e5d",
   "metadata": {},
   "source": [
    "## 6. Generate HTML Page\n",
    "**generate_report**The generate_report module produces a clean, interactive HTML report from processed news data. It loads the final summary, entities, timeline, and cleaned articles, then renders them into structured sections including a hero header, research summary, key entities, event timeline, and news sources. The page features responsive design, Bootstrap styling, and interactive animations.\n",
    "\n",
    "It uses helper functions for robust file loading and dynamically generates HTML cards and grids for clarity. Smooth scrolling and section highlighting enhance user experience. The resulting index.html provides an authoritative, readable view of keyword-related news insights.\n",
    "\n",
    "This module bridges data engineering and front-end presentation, transforming datasets into a polished report. It ensures fault tolerance, maintainable code, and a professional platform for analysis and decision-making."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cae591c-8c05-4d3d-a0e0-939de9bdeec7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from datetime import datetime\n",
    "\n",
    "def load_json_file(filepath):\n",
    "    try:\n",
    "        with open(filepath, 'r', encoding='utf-8') as f:\n",
    "            return json.load(f)\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading {filepath}: {e}\")\n",
    "        return None\n",
    "\n",
    "def load_text_file(filepath):\n",
    "    try:\n",
    "        with open(filepath, 'r', encoding='utf-8') as f:\n",
    "            return f.read()\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading {filepath}: {e}\")\n",
    "        return \"\"\n",
    "\n",
    "def generate_html_page(keywords, summary_text, entities_data, timeline_data, news_data, output_file):\n",
    "    entities_html = \"\"\n",
    "    if entities_data:\n",
    "        if isinstance(entities_data, list):\n",
    "            for award in entities_data:\n",
    "                # Process award name (could be string or list)\n",
    "                prize = award.get('prize', '')\n",
    "                if isinstance(prize, list):\n",
    "                    prize = ', '.join(prize) if prize else 'Unknown Award'\n",
    "                \n",
    "                entities_html += f'''\n",
    "                <div class=\"col-md-6 col-lg-4\">\n",
    "                    <div class=\"entity-card\">\n",
    "                        <h5>{prize}</h5>\n",
    "                        <div class=\"award-details\">'''\n",
    "                \n",
    "                # Process people\n",
    "                people = award.get('people', [])\n",
    "                if people:\n",
    "                    entities_html += '<div class=\"entity-section\"><strong>Recipients:</strong><ul class=\"entity-list\">'\n",
    "                    for person in people:\n",
    "                        entities_html += f'<li>{person}</li>'\n",
    "                    entities_html += '</ul></div>'\n",
    "                \n",
    "                # Process organizations\n",
    "                organizations = award.get('organizations', [])\n",
    "                if organizations:\n",
    "                    entities_html += '<div class=\"entity-section\"><strong>Awarding Institutions:</strong><ul class=\"entity-list\">'\n",
    "                    for org in organizations:\n",
    "                        entities_html += f'<li>{org}</li>'\n",
    "                    entities_html += '</ul></div>'\n",
    "                \n",
    "                entities_html += '''\n",
    "                        </div>\n",
    "                    </div>\n",
    "                </div>'''\n",
    "    \n",
    "        elif isinstance(entities_data, dict):\n",
    "            # Keep original dictionary processing as backup\n",
    "            for category, items in entities_data.items():\n",
    "                entities_html += f'''\n",
    "                <div class=\"col-md-6 col-lg-4\">\n",
    "                    <div class=\"entity-card\">\n",
    "                        <h5>{category}</h5>\n",
    "                        <ul class=\"entity-list\">'''\n",
    "                if isinstance(items, list):\n",
    "                    for item in items:\n",
    "                        entities_html += f'<li>{item}</li>'\n",
    "                entities_html += '''\n",
    "                    </ul>\n",
    "                </div>\n",
    "            </div>'''\n",
    "\n",
    "    timeline_html = \"\"\n",
    "    if timeline_data:\n",
    "        if isinstance(timeline_data, list):\n",
    "            for event in timeline_data:\n",
    "                if isinstance(event, dict):\n",
    "                    date = event.get('date', event.get('time', 'Unknown Date'))\n",
    "                    description = event.get('event', event.get('description', ''))\n",
    "                    timeline_html += f'''\n",
    "                    <div class=\"timeline-item\">\n",
    "                        <div class=\"timeline-content\">\n",
    "                            <span class=\"timeline-date\">{date}</span>\n",
    "                            <p class=\"mb-0\">{description}</p>\n",
    "                        </div>\n",
    "                    </div>\n",
    "                    '''\n",
    "                else:\n",
    "                    timeline_html += f'''\n",
    "                    <div class=\"timeline-item\">\n",
    "                        <div class=\"timeline-content\">\n",
    "                            <p class=\"mb-0\">{event}</p>\n",
    "                        </div>\n",
    "                    </div>'''\n",
    "        elif isinstance(timeline_data, dict):\n",
    "            for date, events in sorted(timeline_data.items()):\n",
    "                if isinstance(events, list):\n",
    "                    for event in events:\n",
    "                        timeline_html += f'''\n",
    "                        <div class=\"timeline-item\">\n",
    "                            <div class=\"timeline-content\">\n",
    "                                <span class=\"timeline-date\">{date}</span>\n",
    "                                <p class=\"mb-0\">{event}</p>\n",
    "                            </div>\n",
    "                        </div>\n",
    "                        '''\n",
    "                else:\n",
    "                    timeline_html += f'''\n",
    "                    <div class=\"timeline-item\">\n",
    "                        <div class=\"timeline-content\">\n",
    "                            <span class=\"timeline-date\">{date}</span>\n",
    "                            <p class=\"mb-0\">{events}</p>\n",
    "                        </div>\n",
    "                    </div>\n",
    "                    '''\n",
    "\n",
    "    sources_html = \"\"\n",
    "    if news_data:\n",
    "        if isinstance(news_data, list):\n",
    "            for idx, article in enumerate(news_data, 10):\n",
    "                if isinstance(article, dict):\n",
    "                    title = article.get('title', f'Article {idx}')\n",
    "                    url = article.get('url', article.get('link', '#'))\n",
    "\n",
    "                    sources_html += f'''\n",
    "                <div class=\"col-md-6\">\n",
    "                    <div class=\"card source-card custom-card\">\n",
    "                        <div class=\"card-body\">\n",
    "                            <h5 class=\"card-title\">{title}</h5>\n",
    "                            <a href=\"{url}\" target=\"_blank\" class=\"btn btn-outline-primary btn-sm\">Read Article →</a>\n",
    "                        </div>\n",
    "                    </div>\n",
    "                </div>'''\n",
    "\n",
    "    html_template = f'''<!DOCTYPE html>\n",
    "<html lang=\"en\">\n",
    "<head>\n",
    "    <meta charset=\"UTF-8\">\n",
    "    <meta name=\"viewport\" content=\"width=device-width, initial-scale=1.0\">\n",
    "    <title>News Topic Summary - {keywords}</title>\n",
    "    \n",
    "    <!-- Bootstrap CSS - Local File -->\n",
    "    <link rel=\"stylesheet\" href=\"bootstrap.min.css\">\n",
    "    \n",
    "    <!-- Custom Styles -->\n",
    "    <style>\n",
    "        :root {{\n",
    "            --primary-color: #0d6efd;\n",
    "            --secondary-color: #6c757d;\n",
    "            --success-color: #198754;\n",
    "            --info-color: #0dcaf0;\n",
    "            --warning-color: #ffc107;\n",
    "            --danger-color: #dc3545;\n",
    "            --dark-color: #212529;\n",
    "            --light-color: #f8f9fa;\n",
    "        }}\n",
    "\n",
    "        body {{\n",
    "            font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', 'Roboto', 'Helvetica Neue', Arial, sans-serif;\n",
    "        }}\n",
    "\n",
    "        /* Top Bar */\n",
    "        .top-bar {{\n",
    "            background: var(--dark-color);\n",
    "            color: white;\n",
    "            padding: 10px 0;\n",
    "            font-size: 0.9rem;\n",
    "        }}\n",
    "\n",
    "        .top-bar a {{\n",
    "            color: white;\n",
    "            text-decoration: none;\n",
    "            margin-left: 15px;\n",
    "        }}\n",
    "\n",
    "        .top-bar a:hover {{\n",
    "            color: var(--info-color);\n",
    "        }}\n",
    "\n",
    "        /* Custom Navbar */\n",
    "        .navbar-brand {{\n",
    "            font-weight: 700;\n",
    "            font-size: 1.5rem;\n",
    "        }}\n",
    "\n",
    "        .navbar {{\n",
    "            box-shadow: 0 2px 10px rgba(0,0,0,0.1);\n",
    "        }}\n",
    "\n",
    "        /* Hero Section */\n",
    "        .hero-section {{\n",
    "            background: linear-gradient(135deg, #0d6efd 0%, #0a58ca 100%);\n",
    "            color: white;\n",
    "            padding: 100px 0;\n",
    "            position: relative;\n",
    "            overflow: hidden;\n",
    "        }}\n",
    "\n",
    "        .hero-section::before {{\n",
    "            content: '';\n",
    "            position: absolute;\n",
    "            top: 0;\n",
    "            left: 0;\n",
    "            right: 0;\n",
    "            bottom: 0;\n",
    "            background: url('data:image/svg+xml,<svg width=\"100\" height=\"100\" xmlns=\"http://www.w3.org/2000/svg\"><defs><pattern id=\"grid\" width=\"100\" height=\"100\" patternUnits=\"userSpaceOnUse\"><path d=\"M 100 0 L 0 0 0 100\" fill=\"none\" stroke=\"rgba(255,255,255,0.1)\" stroke-width=\"1\"/></pattern></defs><rect width=\"100%\" height=\"100%\" fill=\"url(%23grid)\"/></svg>');\n",
    "            opacity: 0.3;\n",
    "        }}\n",
    "\n",
    "        .hero-content {{\n",
    "            position: relative;\n",
    "            z-index: 1;\n",
    "        }}\n",
    "\n",
    "        .hero-section h1 {{\n",
    "            font-size: 3.5rem;\n",
    "            font-weight: 700;\n",
    "            margin-bottom: 20px;\n",
    "        }}\n",
    "\n",
    "        .hero-section p {{\n",
    "            font-size: 1.3rem;\n",
    "            margin-bottom: 30px;\n",
    "        }}\n",
    "\n",
    "        .stat-card {{\n",
    "            background: rgba(255,255,255,0.1);\n",
    "            backdrop-filter: blur(10px);\n",
    "            border-radius: 10px;\n",
    "            padding: 30px;\n",
    "            text-align: center;\n",
    "            border: 1px solid rgba(255,255,255,0.2);\n",
    "        }}\n",
    "\n",
    "        .stat-number {{\n",
    "            font-size: 3rem;\n",
    "            font-weight: 700;\n",
    "            display: block;\n",
    "            margin-bottom: 10px;\n",
    "        }}\n",
    "\n",
    "        /* Section Headers */\n",
    "        .section-header {{\n",
    "            text-align: center;\n",
    "            margin-bottom: 50px;\n",
    "        }}\n",
    "\n",
    "        .section-header h2 {{\n",
    "            font-size: 2.5rem;\n",
    "            font-weight: 700;\n",
    "            color: var(--dark-color);\n",
    "            margin-bottom: 15px;\n",
    "            position: relative;\n",
    "            display: inline-block;\n",
    "        }}\n",
    "\n",
    "        .section-header h2::after {{\n",
    "            content: '';\n",
    "            position: absolute;\n",
    "            bottom: -10px;\n",
    "            left: 50%;\n",
    "            transform: translateX(-50%);\n",
    "            width: 60px;\n",
    "            height: 4px;\n",
    "            background: var(--primary-color);\n",
    "            border-radius: 2px;\n",
    "        }}\n",
    "\n",
    "        .section-header p {{\n",
    "            color: var(--secondary-color);\n",
    "            font-size: 1.1rem;\n",
    "        }}\n",
    "\n",
    "        /* Cards */\n",
    "        .custom-card {{\n",
    "            transition: transform 0.3s, box-shadow 0.3s;\n",
    "            height: 100%;\n",
    "            border: none;\n",
    "            box-shadow: 0 2px 10px rgba(0,0,0,0.08);\n",
    "        }}\n",
    "\n",
    "        .custom-card:hover {{\n",
    "            transform: translateY(-5px);\n",
    "            box-shadow: 0 5px 20px rgba(0,0,0,0.15);\n",
    "        }}\n",
    "\n",
    "        /* Timeline */\n",
    "        .timeline {{\n",
    "            position: relative;\n",
    "            padding: 20px 0;\n",
    "        }}\n",
    "\n",
    "        .timeline::before {{\n",
    "            content: '';\n",
    "            position: absolute;\n",
    "            left: 50%;\n",
    "            top: 0;\n",
    "            bottom: 0;\n",
    "            width: 2px;\n",
    "            background: var(--primary-color);\n",
    "            transform: translateX(-50%);\n",
    "        }}\n",
    "\n",
    "        .timeline-item {{\n",
    "            position: relative;\n",
    "            margin-bottom: 50px;\n",
    "            width: 45%;\n",
    "        }}\n",
    "\n",
    "        .timeline-item:nth-child(odd) {{\n",
    "            margin-left: 0;\n",
    "            text-align: right;\n",
    "        }}\n",
    "\n",
    "        .timeline-item:nth-child(even) {{\n",
    "            margin-left: 55%;\n",
    "            text-align: left;\n",
    "        }}\n",
    "\n",
    "        .timeline-item::before {{\n",
    "            content: '';\n",
    "            position: absolute;\n",
    "            width: 20px;\n",
    "            height: 20px;\n",
    "            background: var(--primary-color);\n",
    "            border: 4px solid white;\n",
    "            border-radius: 50%;\n",
    "            box-shadow: 0 0 0 4px var(--primary-color);\n",
    "            top: 20px;\n",
    "        }}\n",
    "\n",
    "        .timeline-item:nth-child(odd)::before {{\n",
    "            right: -60px;\n",
    "        }}\n",
    "\n",
    "        .timeline-item:nth-child(even)::before {{\n",
    "            left: -60px;\n",
    "        }}\n",
    "\n",
    "        .timeline-content {{\n",
    "            background: white;\n",
    "            padding: 20px;\n",
    "            border-radius: 8px;\n",
    "            box-shadow: 0 2px 10px rgba(0,0,0,0.1);\n",
    "        }}\n",
    "\n",
    "        .timeline-date {{\n",
    "            display: inline-block;\n",
    "            background: var(--primary-color);\n",
    "            color: white;\n",
    "            padding: 5px 15px;\n",
    "            border-radius: 20px;\n",
    "            font-size: 0.9rem;\n",
    "            font-weight: 600;\n",
    "            margin-bottom: 10px;\n",
    "        }}\n",
    "\n",
    "        /* Entity Cards */\n",
    "        .entity-card {{\n",
    "            background: var(--light-color);\n",
    "            border-left: 4px solid var(--primary-color);\n",
    "            padding: 25px;\n",
    "            border-radius: 8px;\n",
    "            margin-bottom: 20px;\n",
    "            transition: all 0.3s;\n",
    "        }}\n",
    "\n",
    "        .entity-card:hover {{\n",
    "            background: white;\n",
    "            box-shadow: 0 3px 15px rgba(0,0,0,0.1);\n",
    "        }}\n",
    "\n",
    "        .entity-card h5 {{\n",
    "            color: var(--primary-color);\n",
    "            font-weight: 600;\n",
    "            margin-bottom: 15px;\n",
    "            padding-bottom: 10px;\n",
    "            border-bottom: 2px solid #dee2e6;\n",
    "        }}\n",
    "\n",
    "        .entity-list {{\n",
    "            list-style: none;\n",
    "            padding: 0;\n",
    "        }}\n",
    "\n",
    "        .entity-list li {{\n",
    "            padding: 8px 0;\n",
    "            color: var(--dark-color);\n",
    "            display: flex;\n",
    "            align-items: center;\n",
    "        }}\n",
    "\n",
    "        .entity-list li::before {{\n",
    "            content: '▸';\n",
    "            color: var(--primary-color);\n",
    "            font-weight: bold;\n",
    "            margin-right: 10px;\n",
    "        }}\n",
    "\n",
    "        /* Source Cards */\n",
    "        .source-card {{\n",
    "            border-left: 4px solid var(--success-color);\n",
    "            transition: all 0.3s;\n",
    "        }}\n",
    "\n",
    "        .source-card:hover {{\n",
    "            border-left-color: var(--primary-color);\n",
    "        }}\n",
    "\n",
    "        /* Footer */\n",
    "        footer {{\n",
    "            background: var(--dark-color);\n",
    "            color: white;\n",
    "            padding: 60px 0 20px;\n",
    "            margin-top: 80px;\n",
    "        }}\n",
    "\n",
    "        footer h5 {{\n",
    "            color: white;\n",
    "            font-weight: 600;\n",
    "            margin-bottom: 20px;\n",
    "        }}\n",
    "\n",
    "        footer a {{\n",
    "            color: #adb5bd;\n",
    "            text-decoration: none;\n",
    "            transition: color 0.3s;\n",
    "        }}\n",
    "\n",
    "        footer a:hover {{\n",
    "            color: var(--primary-color);\n",
    "        }}\n",
    "\n",
    "        footer .list-unstyled li {{\n",
    "            margin-bottom: 10px;\n",
    "        }}\n",
    "\n",
    "        .footer-bottom {{\n",
    "            border-top: 1px solid #495057;\n",
    "            padding-top: 20px;\n",
    "            margin-top: 40px;\n",
    "            text-align: center;\n",
    "            color: #adb5bd;\n",
    "        }}\n",
    "\n",
    "        /* Responsive Timeline */\n",
    "        @media (max-width: 768px) {{\n",
    "            .hero-section h1 {{\n",
    "                font-size: 2rem;\n",
    "            }}\n",
    "\n",
    "            .timeline::before {{\n",
    "                left: 20px;\n",
    "            }}\n",
    "\n",
    "            .timeline-item {{\n",
    "                width: 100%;\n",
    "                margin-left: 40px !important;\n",
    "                text-align: left !important;\n",
    "            }}\n",
    "\n",
    "            .timeline-item::before {{\n",
    "                left: -30px !important;\n",
    "            }}\n",
    "        }}\n",
    "\n",
    "        /* Animations */\n",
    "        .fade-in {{\n",
    "            animation: fadeIn 0.8s ease-in;\n",
    "        }}\n",
    "\n",
    "        @keyframes fadeIn {{\n",
    "            from {{\n",
    "                opacity: 0;\n",
    "                transform: translateY(20px);\n",
    "            }}\n",
    "            to {{\n",
    "                opacity: 1;\n",
    "                transform: translateY(0);\n",
    "            }}\n",
    "        }}\n",
    "    </style>\n",
    "</head>\n",
    "<body>\n",
    "    <!-- Top Bar -->\n",
    "    <div class=\"top-bar\">\n",
    "        <div class=\"container\">\n",
    "            <div class=\"row\">\n",
    "                <div class=\"col-md-6\">\n",
    "                    <span>contact@newsresearch.org</span>\n",
    "                    <span class=\"ms-3\">+1 (555) 123-4567</span>\n",
    "                </div>\n",
    "                <div class=\"col-md-6 text-end\">\n",
    "                    <a href=\"#\">中文</a>\n",
    "                    <a href=\"#\">English</a>\n",
    "                </div>\n",
    "            </div>\n",
    "        </div>\n",
    "    </div>\n",
    "\n",
    "    <!-- Navigation Bar -->\n",
    "    <nav class=\"navbar navbar-expand-lg navbar-light bg-white sticky-top\">\n",
    "        <div class=\"container\">\n",
    "            <a class=\"navbar-brand\" href=\"#\">\n",
    "                <span class=\"badge bg-primary me-2\">NR</span>\n",
    "                {keywords} News Summary\n",
    "            </a>\n",
    "            <button class=\"navbar-toggler\" type=\"button\" data-bs-toggle=\"collapse\" data-bs-target=\"#navbarNav\">\n",
    "                <span class=\"navbar-toggler-icon\"></span>\n",
    "            </button>\n",
    "            <div class=\"collapse navbar-collapse\" id=\"navbarNav\">\n",
    "                <ul class=\"navbar-nav ms-auto\">\n",
    "                    <li class=\"nav-item\">\n",
    "                        <a class=\"nav-link active\" href=\"#summary\">Research Report</a>\n",
    "                    </li>\n",
    "                    <li class=\"nav-item\">\n",
    "                        <a class=\"nav-link\" href=\"#entities\">Key Entities</a>\n",
    "                    </li>\n",
    "                    <li class=\"nav-item\">\n",
    "                        <a class=\"nav-link\" href=\"#timeline\">Event Timeline</a>\n",
    "                    </li>\n",
    "                    <li class=\"nav-item\">\n",
    "                        <a class=\"nav-link\" href=\"#sources\">Resources</a>\n",
    "                    </li>\n",
    "                </ul>\n",
    "            </div>\n",
    "        </div>\n",
    "    </nav>\n",
    "\n",
    "    <!-- Hero Section -->\n",
    "    <section class=\"hero-section\">\n",
    "        <div class=\"container hero-content\">\n",
    "            <div class=\"row\">\n",
    "                <div class=\"col-lg-8 mx-auto text-center\">\n",
    "                    <h1 class=\"fade-in\">News Topic Comprehensive Report</h1>\n",
    "                    <p class=\"lead fade-in\">In-depth Analysis · Authoritative Sources · Professional Insights</p>\n",
    "                </div>\n",
    "            </div>\n",
    "        </div>\n",
    "    </section>\n",
    "\n",
    "    <!-- Breadcrumb Navigation -->\n",
    "    <nav aria-label=\"breadcrumb\" class=\"bg-light py-3\">\n",
    "        <div class=\"container\">\n",
    "            <ol class=\"breadcrumb mb-0\">\n",
    "                <li class=\"breadcrumb-item\"><a href=\"#\">Home</a></li>\n",
    "                <li class=\"breadcrumb-item\"><a href=\"#\">Research Reports</a></li>\n",
    "                <li class=\"breadcrumb-item active\">News Topic Report</li>\n",
    "            </ol>\n",
    "        </div>\n",
    "    </nav>\n",
    "\n",
    "    <!-- Main Summary Section -->\n",
    "    <section id=\"summary\" class=\"py-5\">\n",
    "        <div class=\"container\">\n",
    "            <div class=\"section-header\">\n",
    "                <h2>Research Report Summary</h2>\n",
    "                <p>In-depth analysis of breakthrough achievements in the news topic</p>\n",
    "            </div>\n",
    "            \n",
    "            <div class=\"row g-4\">\n",
    "                <div class=\"col-12\">\n",
    "                    <div class=\"card custom-card\">\n",
    "                        <div class=\"card-body\">\n",
    "                            <div style=\"white-space: pre-wrap; line-height: 1.8;\">{summary_text if summary_text else 'No summary content available'}</div>\n",
    "                        </div>\n",
    "                    </div>\n",
    "                </div>\n",
    "            </div>\n",
    "        </div>\n",
    "    </section>\n",
    "\n",
    "    <!-- Key Entities Section -->\n",
    "    <section id=\"entities\" class=\"py-5 bg-light\">\n",
    "        <div class=\"container\">\n",
    "            <div class=\"section-header\">\n",
    "                <h2>Key Entities Directory</h2>\n",
    "                <p>Important people and organizations related to the news topic</p>\n",
    "            </div>\n",
    "            \n",
    "            <div class=\"row g-4\">\n",
    "                {entities_html if entities_html else '<div class=\"col-12\"><p class=\"text-muted text-center\">No entity data available</p></div>'}\n",
    "            </div>\n",
    "        </div>\n",
    "    </section>\n",
    "\n",
    "    <!-- Timeline Section -->\n",
    "    <section id=\"timeline\" class=\"py-5\">\n",
    "        <div class=\"container\">\n",
    "            <div class=\"section-header\">\n",
    "                <h2>Event Timeline</h2>\n",
    "                <p>Chronology of major events</p>\n",
    "            </div>\n",
    "            \n",
    "            <div class=\"timeline\">\n",
    "                {timeline_html if timeline_html else '<p class=\"text-muted text-center\">No timeline data available</p>'}\n",
    "            </div>\n",
    "        </div>\n",
    "    </section>\n",
    "\n",
    "    <!-- Resources Section -->\n",
    "    <section id=\"sources\" class=\"py-5 bg-light\">\n",
    "        <div class=\"container\">\n",
    "            <div class=\"section-header\">\n",
    "                <h2>Resource Center</h2>\n",
    "                <p>Authoritative news sources and reference materials</p>\n",
    "            </div>\n",
    "            \n",
    "            <div class=\"row g-4\">\n",
    "                {sources_html if sources_html else '<div class=\"col-12\"><p class=\"text-muted text-center\">No news source data available</p></div>'}\n",
    "            </div>\n",
    "        </div>\n",
    "    </section>\n",
    "\n",
    "    <!-- Footer -->\n",
    "    <footer>\n",
    "        <div class=\"container\">\n",
    "            <div class=\"row\">\n",
    "                <div class=\"col-lg-3 col-md-6 mb-4\">\n",
    "                    <h5>About Us</h5>\n",
    "                    <p>News Research Center is committed to providing authoritative and professional news research reports and in-depth analysis.</p>\n",
    "                </div>\n",
    "                <div class=\"col-lg-3 col-md-6 mb-4\">\n",
    "                    <h5>Quick Links</h5>\n",
    "                    <ul class=\"list-unstyled\">\n",
    "                        <li><a href=\"#summary\">Research Report</a></li>\n",
    "                        <li><a href=\"#entities\">Key Entities</a></li>\n",
    "                        <li><a href=\"#timeline\">Event Timeline</a></li>\n",
    "                        <li><a href=\"#sources\">Resources</a></li>\n",
    "                    </ul>\n",
    "                </div>\n",
    "                <div class=\"col-lg-3 col-md-6 mb-4\">\n",
    "                    <h5>Contact Information</h5>\n",
    "                    <ul class=\"list-unstyled\">\n",
    "                        <li>contact@newsresearch.org</li>\n",
    "                        <li>+1 (555) 123-4567</li>\n",
    "                        <li>New York, USA</li>\n",
    "                    </ul>\n",
    "                </div>\n",
    "                <div class=\"col-lg-3 col-md-6 mb-4\">\n",
    "                    <h5>Data Sources</h5>\n",
    "                    <p>This report data comes from multiple authoritative news agencies, collected, cleaned and analyzed through automated data pipelines.</p>\n",
    "                </div>\n",
    "            </div>\n",
    "            <div class=\"footer-bottom\">\n",
    "                <p>&copy; 2025 News Research Center. All rights reserved. | This page was automatically generated by data engineering pipeline | Generated: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}</p>\n",
    "            </div>\n",
    "        </div>\n",
    "    </footer>\n",
    "\n",
    "    <!-- Bootstrap JS - Local File -->\n",
    "    <script src=\"bootstrap.bundle.min.js\"></script>\n",
    "    \n",
    "    <!-- Custom Scripts -->\n",
    "    <script>\n",
    "        // Smooth scrolling\n",
    "        document.querySelectorAll('a[href^=\"#\"]').forEach(anchor => {{\n",
    "            anchor.addEventListener('click', function (e) {{\n",
    "                e.preventDefault();\n",
    "                const target = document.querySelector(this.getAttribute('href'));\n",
    "                if (target) {{\n",
    "                    const headerOffset = 70;\n",
    "                    const elementPosition = target.getBoundingClientRect().top;\n",
    "                    const offsetPosition = elementPosition + window.pageYOffset - headerOffset;\n",
    "\n",
    "                    window.scrollTo({{\n",
    "                        top: offsetPosition,\n",
    "                        behavior: 'smooth'\n",
    "                    }});\n",
    "                }}\n",
    "            }});\n",
    "        }});\n",
    "\n",
    "        // Navbar active state\n",
    "        window.addEventListener('scroll', () => {{\n",
    "            let current = '';\n",
    "            const sections = document.querySelectorAll('section[id]');\n",
    "            \n",
    "            sections.forEach(section => {{\n",
    "                const sectionTop = section.offsetTop;\n",
    "                if (pageYOffset >= sectionTop - 100) {{\n",
    "                    current = section.getAttribute('id');\n",
    "                }}\n",
    "            }});\n",
    "\n",
    "            document.querySelectorAll('.navbar-nav .nav-link').forEach(link => {{\n",
    "                link.classList.remove('active');\n",
    "                if (link.getAttribute('href') === `#${{current}}`) {{\n",
    "                    link.classList.add('active');\n",
    "                }}\n",
    "            }});\n",
    "        }});\n",
    "\n",
    "        // Scroll animations\n",
    "        const observerOptions = {{\n",
    "            threshold: 0.1,\n",
    "            rootMargin: '0px 0px -50px 0px'\n",
    "        }};\n",
    "\n",
    "        const observer = new IntersectionObserver((entries) => {{\n",
    "            entries.forEach(entry => {{\n",
    "                if (entry.isIntersecting) {{\n",
    "                    entry.target.style.opacity = '1';\n",
    "                    entry.target.style.transform = 'translateY(0)';\n",
    "                }}\n",
    "            }});\n",
    "        }}, observerOptions);\n",
    "\n",
    "        document.querySelectorAll('.custom-card, .entity-card, .timeline-item').forEach(el => {{\n",
    "            el.style.opacity = '0';\n",
    "            el.style.transform = 'translateY(20px)';\n",
    "            el.style.transition = 'opacity 0.6s ease, transform 0.6s ease';\n",
    "            observer.observe(el);\n",
    "        }});\n",
    "    </script>\n",
    "</body>\n",
    "</html>'''\n",
    "\n",
    "    try:\n",
    "        with open(output_file, 'w', encoding='utf-8') as f:\n",
    "            f.write(html_template)\n",
    "        print(f\"HTML page successfully generated: {output_file}\")\n",
    "        return True\n",
    "    except Exception as e:\n",
    "        print(f\"Error generating HTML file: {e}\")\n",
    "        return False\n",
    "\n",
    "def generate_report(keywords):\n",
    "    print(\"Starting HTML summary page generation...\")\n",
    "    print(\"Loading data files...\")\n",
    "    \n",
    "    # Load data files from DataEngineering5481/DataEngineering directory\n",
    "    summary_text = load_text_file('final_summary.txt')\n",
    "    entities_data = load_json_file('entities.json')\n",
    "    timeline_data = load_json_file('timeline.json')\n",
    "    news_data = load_json_file('cleaned_news.json')\n",
    "\n",
    "    success = generate_html_page(\n",
    "        keywords,\n",
    "        summary_text=summary_text,\n",
    "        entities_data=entities_data,\n",
    "        timeline_data=timeline_data,\n",
    "        news_data=news_data,\n",
    "        output_file='index.html'\n",
    "    )\n",
    "\n",
    "    if success:\n",
    "        print(\"All operations completed!\")\n",
    "        print(\"Tip: Open index.html in your browser to view the results\")\n",
    "        print(\"Local Bootstrap files:\")\n",
    "        print(\"  - bootstrap.min.css\")\n",
    "        print(\"  - bootstrap.bundle.min.js\")\n",
    "    else:\n",
    "        print(\"Issues encountered during generation, please check error messages\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34d750ca-5c8f-42c7-87b0-7e929fe3e319",
   "metadata": {},
   "source": [
    "## 7. Run pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7be47e0a-5dea-45d2-a922-11a2afc17bb4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-01T11:55:16.882439Z",
     "start_time": "2025-11-01T11:55:16.879297Z"
    }
   },
   "outputs": [],
   "source": [
    "# Input event name:\n",
    "keyword = \"Nobel Prize\" # 2025 Nobel Prize\n",
    "# Input start time:\n",
    "start_time = \"2025-09-23\" # “2025-09-23” \n",
    "# Input end time:\n",
    "end_time = \"2025-10-22\" # “2025-10-22”"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a344d13-4d9c-4135-b266-164ca2a6164f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crawl the news\n",
    "crawl_the_news(keyword, start_time, end_time)\n",
    "# Clean the data\n",
    "cleaned_the_data()\n",
    "\n",
    "# Extract information\n",
    "with open(\"cleaned_news.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    data = json.load(f)\n",
    "# Extract timeline\n",
    "timeline = extract_timeline(data,keyword)\n",
    "# save results\n",
    "json.dump(timeline, open(\"timeline.json\", \"w\", encoding=\"utf-8\"), ensure_ascii=False, indent=2)\n",
    "# Extract entities\n",
    "entities = extract_entities(data,keyword)\n",
    "# save results\n",
    "json.dump(entities, open(\"entities.json\", \"w\", encoding=\"utf-8\"), ensure_ascii=False, indent=2)\n",
    "\n",
    "# Summarize\n",
    "summary(keyword)\n",
    "\n",
    "# Generate HTML page\n",
    "generate_report(keyword)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
